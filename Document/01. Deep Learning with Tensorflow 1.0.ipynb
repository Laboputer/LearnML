{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Learning with TensorFlow 1.0 (Python) !**\n",
    " > Deep Learning을 이해하기 위해 인터넷강의를 통해 공부하고, TensorFlow 1.0으로 직접 구현해보면서\n",
    " 학습한 내용을 토대로 작성되었다.\n",
    " 이 자료를 통해 기초적인 Machine Learning에 대한 개념과 Deep Learning을 이해하고, 실제 데이터에 대한 예측 모델을 구성해볼 수 있다.\n",
    " \n",
    "*참고자료* :  \n",
    "*[김성훈 님][https://github.com/hunkim]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **====== Contents ======**\n",
    "\n",
    "### 0. Machine Learning Basic concepts\n",
    "### 1. Linear Regression\n",
    "    1-1 Linear Regression (Tensorflow)\n",
    "    1-2 Linear Regression (Cost Function이 Convex 할까?)\n",
    "    1-3 Linear Regression (Multi-variable은 어떻게 계산할까? - 행렬연산)\n",
    "### 2. Logistic Regression (classification)\n",
    "    2-1 Logistic Regression (Tensorflow)\n",
    "    2-2 Softmax Classification\n",
    "### 3. NN (Neural Network)\n",
    "    3-1 XOR Problem (Linear Regression, Neural Network)\n",
    "    3-2 XOR with Deep-Wide Learning (Vanishing gradient)\n",
    "    (Bonus) 실제 데이터를 사용하기 전 알아야할 개념\n",
    "### (실습1) 다양한 모델을 이용한 MNIST 예측하기\n",
    "    실습1-1) Logistic Classification (Regression)\n",
    "    실습1-2) Logistic Classification (Softmax)\n",
    "    실습1-3) Deep + Wide NN\n",
    "    실습1-4) ReLU, Xavier, Dropout, AdamOptimizer\n",
    "### 4. CNN (Convolutional N.N.)\n",
    "### (실습2) CNN을 이용하여 MNIST 이미지 인식률 향상\n",
    "### 5. RNN (Recurrent N.N.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Machine Learning Basic concepts\n",
    "![Machine Learning](Image/image1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #1. Check TF Version\n",
      "1.12.0\n",
      "\n",
      " #2. Hello Tensorflow\n",
      "b'Hello, TensorFlow!'\n",
      "\n",
      " #3. Computational Graph\n",
      "node1: Tensor(\"Const_10:0\", shape=(), dtype=float32)\n",
      "node2: Tensor(\"Const_11:0\", shape=(), dtype=float32)\n",
      "node3:  Tensor(\"Add_6:0\", shape=(), dtype=float32)\n",
      "sess.run(node1, node2):  [3.0, 4.0]\n",
      "sess.run(node3):  7.0\n",
      "\n",
      " #4. Placeholder\n",
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "# Getting Started With TensorFlow\n",
    "# https://www.tensorflow.org/get_started/get_started\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "## 1. Check TF Verison\n",
    "print(\" #1. Check TF Version\")\n",
    "print(tf.__version__)\n",
    "\n",
    "## 2. Hello Tensorflow\n",
    "# Create a constant op\n",
    "# This op is added as a node to the default graph\n",
    "print(\"\\n #2. Hello Tensorflow\")\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "# start a TF session\n",
    "sess = tf.Session()\n",
    "# run the op and get result\n",
    "print(sess.run(hello))\n",
    "\n",
    "## 3. Computational Graph\n",
    "print(\"\\n #3. Computational Graph\")\n",
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "node3 = tf.add(node1, node2)\n",
    "\n",
    "print(\"node1:\", node1)\n",
    "print(\"node2:\", node2)\n",
    "print(\"node3: \", node3)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(\"sess.run(node1, node2): \", sess.run([node1, node2]))\n",
    "print(\"sess.run(node3): \", sess.run(node3))\n",
    "\n",
    "## 4. Placeholder\n",
    "print(\"\\n #4. Placeholder\")\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, feed_dict={a: [1,3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "\n",
    "> Linear Regression은 종속 변수 Y와 한 개 이상의 독립 변수 X와의 선형 상관 관계를 모델링하는 회귀분석 기법이다.\n",
    "\n",
    "![Machine Learning](Image/image2.jpg)\n",
    "\n",
    "* **Hypothesis**\n",
    "  \n",
    "  Weight * Features 에 해당하는 모델을 구성한다.\n",
    "\n",
    "* **Cost Function**\n",
    "\n",
    "  Hypothesis에 할당된 Weight값에 의해 어떤 Cost(Loss)를 가지는지 계산하는 함수이며 이 Cost Function \n",
    "  이 최소화하는 Weight들을 검색하는 것이 우리의 목표이다.\n",
    "\n",
    "* **Gradient descent algorithm**\n",
    "\n",
    "  Cost를 최소화하기 위해 cost(W)에 대해 편미분한 값을 이용하여 최저점을 탐색하는 알고리즘\n",
    "\n",
    "* **Convex function**\n",
    "\n",
    "  볼록한 형태의 함수로, Local minimum에 빠지지 않고, 미분한 값이 0에 가까울수록 Global Minimum에 해당하는 값을 도출할 수 있는 형태이다. Cost Function이 이 형태를 띄어야 학습이 제대로 동작할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1 Linear Regression (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1-1. Linear Regression (Tesorflow). Learing Started.\n",
      "Step : 0, Cost : 28.97908, [W, b] = [-0.029, 0.341]\n",
      "Step : 50, Cost : 0.02526, [W, b] = [1.103, 0.729]\n",
      "Step : 100, Cost : 0.01801, [W, b] = [1.087, 0.787]\n",
      "Step : 150, Cost : 0.01283, [W, b] = [1.073, 0.835]\n",
      "Step : 200, Cost : 0.00915, [W, b] = [1.062, 0.877]\n",
      "Step : 250, Cost : 0.00652, [W, b] = [1.052, 0.911]\n",
      "Step : 300, Cost : 0.00465, [W, b] = [1.044, 0.941]\n",
      "Step : 350, Cost : 0.00331, [W, b] = [1.037, 0.966]\n",
      "Step : 400, Cost : 0.00236, [W, b] = [1.031, 0.987]\n",
      "Step : 450, Cost : 0.00168, [W, b] = [1.027, 1.004]\n",
      "Step : 500, Cost : 0.00120, [W, b] = [1.022, 1.019]\n",
      "[6.131133]\n",
      "[3.575126]\n",
      "[2.5527232 4.5975285]\n"
     ]
    }
   ],
   "source": [
    "# 1-1. Linear Regression (Tesorflow)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Traning Set\n",
    "X_data = [1, 2, 3, 4, 5]\n",
    "Y_data = [2.1, 3.1, 4.1, 5.1, 6.1]\n",
    "\n",
    "# Variable : Tensorflow상에서 학습되는 Weight로 Graph Run되면 자동으로 값을 찾는다.\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None])\n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "# Hyposthesis\n",
    "hypothesis = X * W + b\n",
    "# Linear regression Cost Function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"# 1-1. Linear Regression (Tesorflow). Learing Started.\")\n",
    "#Learning\n",
    "for step in range(501):\n",
    "    cost_val, w_val, b_val, _ = sess.run([cost,W,b,train], \n",
    "                                         feed_dict={X: X_data,\n",
    "                                                    Y: Y_data})\n",
    "    if(step % 50 == 0):\n",
    "        print(\"Step : %d, Cost : %.5f, [W, b] = [%.3f, %.3f]\" % (step, cost_val, w_val, b_val))\n",
    "\n",
    "# Testing our model\n",
    "print(sess.run(hypothesis, feed_dict={X:[5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X:[2.5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X:[1.5, 3.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2 Linear Regression (Cost Function이 Convex 할까?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1-2 Linear Regression (Cost Function이 Convex 할까?)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1.2 Linear Regression\n",
    "# Cost Function이 Convex function인지 확인하기\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X=[1,2,3]\n",
    "Y=[1,2,3]\n",
    "\n",
    "W=tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = X * W\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "W_val=[]\n",
    "cost_val=[]\n",
    "\n",
    "print(\"# 1-2 Linear Regression (Cost Function이 Convex 할까?)\")\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i*0.1\n",
    "    curr_cost, curr_W = sess.run([cost,W], feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "\n",
    "# Show the cost function\n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3 Linear Regression (Multi-variable은 어떻게 계산할까? - 행렬연산)\n",
    "![Multi-variable](Image/image3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1.3 Linear Regression(Multi-Variable) Learing Started.\n",
      "Step : 0, cost : 79654.64062, Hypothesis:  [[ -96.312805]\n",
      " [-118.1729  ]\n",
      " [-115.168945]\n",
      " [-125.842766]\n",
      " [ -90.363205]]\n",
      "Step : 200, cost : 3.64365, Hypothesis:  [[153.95853]\n",
      " [182.91168]\n",
      " [181.35086]\n",
      " [197.07372]\n",
      " [139.34601]]\n",
      "Step : 400, cost : 3.29232, Hypothesis:  [[153.81964]\n",
      " [183.00717]\n",
      " [181.30864]\n",
      " [197.04068]\n",
      " [139.47342]]\n",
      "Step : 600, cost : 2.97706, Hypothesis:  [[153.68811]\n",
      " [183.09756]\n",
      " [181.26865]\n",
      " [197.00928]\n",
      " [139.59413]]\n",
      "Step : 800, cost : 2.69411, Hypothesis:  [[153.56358]\n",
      " [183.18318]\n",
      " [181.2308 ]\n",
      " [196.97945]\n",
      " [139.70856]]\n",
      "Step : 1000, cost : 2.44019, Hypothesis:  [[153.44571]\n",
      " [183.26425]\n",
      " [181.195  ]\n",
      " [196.95111]\n",
      " [139.81702]]\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Linear Regression\n",
    "# Multi-Variable인 경우에도 학습하기\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[73, 80, 75], [93, 88, 93], [89, 91, 90], [96,98, 100], [73, 66, 70]]\n",
    "y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "# [None, 3] : 주어지는 데이터가 N개 이며, X 종류(Features)는  3개\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # X1 ~ X3 : 3\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1]) # Y1 : 1\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train  = optimizer.minimize(cost)\n",
    "\n",
    "#launch the graph in a session\n",
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"# 1.3 Linear Regression(Multi-Variable) Learing Started.\")\n",
    "for step in range(1001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={X : x_data, Y : y_data })\n",
    "    if(step % 200 == 0):\n",
    "        print(\"Step : %d, cost : %.5f,\" % (step, cost_val),\"Hypothesis: \", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression (classification)\n",
    "\n",
    "> Logistic Regression은 일반적인 회귀 분석의 목표와 동일하게 종속 변수와 독립 변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용하는 것이다. Logistic Regression에서는 종속 변수가 범주형 데이터로 Classification 기법으로 생각하면 된다. Tensorflow에서는 각 범주형 데이터일 값을 구하는 Regression과 각 범주형 데이터의 확률로 표기하는 Classification으로 구할 것이다.\n",
    "\n",
    "![](Image/image4.jpg)\n",
    "\n",
    "* **Classification**\n",
    "\n",
    "  데이터를 분류함에 있어서 YES or No (1,0) 둘 중 하나를 분류하는 것을 Binary classification이라 불리며, N개의 범주로 분류되는 것을 Multinomial classfication이라고 한다.\n",
    "  \n",
    "* **Logistic Hypothesis**\n",
    "\n",
    "  Logictic은 Linear Hypothesis 모델을 그대로 이용할 경우 매우 큰 값/작은 값에 평균 값이 크게 달라지는 현상을 보이기 때문에, 데이터를 학습할 때 특이 케이스에 의해 동작이 잘 되지 않는다. 이때에도 잘 동작할 수 있게 하는 것이 매우 큰 이슈였는데, Sigmoid라는 함수를 한번 더 통과함으로써 효과적으로 동작할 수 있다. 이 값은 항상 0~1 사이의 값을 가진다.\n",
    "  \n",
    "* **Cost Function**\n",
    "\n",
    "  Cost Function 또한 그래프처럼 Convex 형태를 띄지 않아 학습이 되지 않는다. 이를 보완하기 위해 X가 지수에 있는 것을 상쇄하기 위해, log을 취한 형태로 Logistic을 위한 새로운 Cost Function을 사용한다. 이 Cost Function을 다시 Gradient decent algorithm을 적용하면 Cost를 최소화할 수 있다.\n",
    "  \n",
    "  \n",
    "  #  \n",
    "  \n",
    "\n",
    "![](Image/image5.jpg)\n",
    "* **Multinomial Classifiaction**\n",
    " \n",
    "  Multinomial Classification은 사실 Binary를 N번 구하는 것으로 구할 수 있다. 이를 행렬곱을 이용하면 쉽게 N번을 연산할 수 있게 된다. 각각의 Score를 계산할 수 있다.\n",
    "  \n",
    "* **Softmax**\n",
    "\n",
    "  Binary Classification에서는 Score에 대해 Sigmoid를 취하면서 0~1 사이의 값을 가지도록 하여 결정하였는데, Multinomial Classification에서 N번 Sigmoid를 취해도 되지만 더 간단한 방법으로 Softmax로 처리할 수 있다. 심지어 Softmax의 결과값이 총합이 1이어서, 각각 경우의 확률과 같다.\n",
    "  \n",
    "* **Cross entropy**\n",
    "\n",
    "  Softmax의 Cost function으로 실제 Y값과 -log(Y 예측값)을 Elementwise 곱한 후 Sumation한 값이다. Logisitic Cost function과 같은 개념이다. - (이유: Logistic에서 one-hot encoding 벡터로 변환한 후 Cross Entropy를 적용하면 같다)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1 Logistic Regression (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 2-1 Logistic Regression (Tensorflow) Learing Started.\n",
      "Step :    0, Cost: 1.03770, Accuracy: 33.33 %\n",
      "Step : 1000, Cost: 0.40463, Accuracy: 83.33 %\n",
      "Step : 2000, Cost: 0.34070, Accuracy: 83.33 %\n",
      "Step : 3000, Cost: 0.29437, Accuracy: 83.33 %\n",
      "Step : 4000, Cost: 0.25807, Accuracy: 83.33 %\n",
      "Step : 5000, Cost: 0.22916, Accuracy: 100.00 %\n",
      "Step : 6000, Cost: 0.20577, Accuracy: 100.00 %\n",
      "Step : 7000, Cost: 0.18658, Accuracy: 100.00 %\n",
      "\n",
      " Hypothesis :  [[0.04774142]\n",
      " [0.17698301]\n",
      " [0.3779481 ]\n",
      " [0.7496976 ]\n",
      " [0.9178857 ]\n",
      " [0.9731891 ]] \n",
      "Correct(Y) :  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      " Accuracy:  100.0\n"
     ]
    }
   ],
   "source": [
    "# 2-1 Logistic Regression (Tensorflow)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))\n",
    "hypothesis =  tf.sigmoid(tf.matmul(X, W) + b)  \n",
    "# Simplified cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train  = optimizer.minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5  => sigmoid를 통과한 값이 0.5보다 크면 1로 인식하게 한다.\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) * 100 # to %\n",
    "\n",
    "print(\"# 2-1 Logistic Regression (Tensorflow) Learing Started.\")\n",
    "#launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(7001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X : x_data, Y : y_data })\n",
    "        if(step % 1000 == 0):\n",
    "            a = sess.run([accuracy],feed_dict={X: x_data, Y:y_data})[0]\n",
    "            print(\"Step : %4d, Cost: %.5f, Accuracy: %.2f\" % (step, cost_val, a), \"%\")\n",
    "\n",
    "\n",
    "    #Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],feed_dict={X: x_data, Y:y_data})\n",
    "    print(\"\\n Hypothesis : \", h, \"\\nCorrect(Y) : \", c, \"\\n Accuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2 Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 2-2 Softmax Classification + Fancy Softmax. Learing Started.\n",
      "Step :    0, Cost: 4.26827, Accuracy: 37.50 %\n",
      "Step :  500, Cost: 0.47780, Accuracy: 75.00 %\n",
      "Step : 1000, Cost: 0.29798, Accuracy: 87.50 %\n",
      "Step : 1500, Cost: 0.20561, Accuracy: 100.00 %\n",
      "Step : 2000, Cost: 0.16757, Accuracy: 100.00 %\n",
      "[[1.4185148e-02 9.8580486e-01 1.0000229e-05]\n",
      " [7.8595561e-01 1.9688424e-01 1.7160088e-02]\n",
      " [2.6808090e-08 4.2461287e-04 9.9957532e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# 2-2 Softmax Classification + Fancy Softmax\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]] # one-hot encoding\n",
    "n_inputs = 4 \n",
    "n_classes = 3\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_inputs])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "W = tf.Variable(tf.random_normal([n_inputs, n_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([n_classes]), name='bias')\n",
    "\n",
    "## 1. Softmax\n",
    "# # tf.nn.softmax computes softmax activations\n",
    "# # softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "# hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "# # Cross entropy cost/loss\n",
    "# cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "## 2. Fancy Softmax with cross entropy\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels= y_data)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# accuracy\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(y_data, 1)) # [예측값, 실제값] 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32)) * 100 # %\n",
    "\n",
    "print(\"# 2-2 Softmax Classification + Fancy Softmax. Learing Started.\")\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        cost_val, _ = sess.run([cost, optimizer], feed_dict={X: x_data, Y:y_data})\n",
    "        if (step % 500) == 0:\n",
    "            a = sess.run(accuracy, feed_dict={X: x_data, Y:y_data})\n",
    "            print(\"Step : %4d, Cost: %.5f, Accuracy: %.2f\" % (step, cost_val, a), \"%\")\n",
    "\n",
    "    # Test & One-hot encoding\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0 , 1]]})\n",
    "    print(all, sess.run(tf.arg_max(all, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. NN (Neural Network)\n",
    "\n",
    "> Neural Network가 등장하게 된 배경과 Neural Network의 Layer를 여러층 두게 될 때 어떻게 학습을 시키는지, 그리고 Machine Learning을 함에 있어서 여태까지 어떤 이슈가 있었는지, 아직 해결안된 문제는 무엇인지 확인해보고자 한다. 먼저 Neural Network의 발전과정을 알아보고, 어떻게 동작하는지 확인해보자.\n",
    "\n",
    "![](Image/image6.jpg)\n",
    "![](Image/image7.jpg)\n",
    "\n",
    "* **Neuron (Activation Functions)**\n",
    "\n",
    " 우리의 뇌 신경계의 세포인 뉴런을 분석해보면 어떤 특정 전기신호가 Input으로 들어오게 되고 일정값이 넘어가면 활성화가 되는 것을 본따서 Network를 구성하였다. 이때 활성화가 되기 위한 조건을 판단하는 함수를 Activation Function이라고 한다.\n",
    " \n",
    "* **Perceptrons**\n",
    " \n",
    " Marvin Minsky(인공지능 분야 교수)는 Perceptrons 이라는 책을 통해 XOR 문제를 한가지 Layer로는 절대 풀 수 없음을 수학적으로 증명하고, Multi Layer를 이용하면 XOR 문제를 해결할 수 있으나, 1969년 당시 어느 누구도 각각의 Layer의 Weight를 컴퓨터로 계산할 수 없음을 밝혔다.\n",
    "  \n",
    "* **Backpropagation**\n",
    " \n",
    " 1974년도 Paul Werbos 라는 분이 Backpropagation을 소개하면서 Multi Layer의 Weight를 계산할 수 있음을 보였고, 주목받지 못하다가 1986년 Hinton이 다시 한번 발표하였는데 꽤 잘 동작하였다.\n",
    "  \n",
    "* **The Problem with Back-Propagation**\n",
    "   \n",
    " Back-Propagation을 이용하여 Multi Layer도 잘 학습되는 듯 하였으나, 적은 Layer에서는 잘 동작하지만 10개 이상 Deep 해질수록 신호가 약해지면서 학습이 제대로 되지 않는 문제가 발생했다. 이때 부터는 SVM, RandomForest 같은 다른 learning algorithm이 떠오르기 시작하였다.\n",
    "  \n",
    "* **Geoffrey Hinton's summary of findings up to today**\n",
    " \n",
    " 여태까지 Deep Layer가 잘 학습되지 않았던 이유 4가지를 들면서, 이를 어느정도 보완한 방법들이 나오자 Deep Learning은 잘 동작하기 시작했다. 간단하게 요약하면 Activation Function을 sigmoid로 그동안 잘 못 사용해왔고, 초기값 설정을 제대로 하지 않고 랜덤하게 셋팅한 것과 컴퓨터가 학습하기에 느렸고, DataSet이 너무 적었다고 말한다. 아래에서 자세하게 다뤄보자.\n",
    "\n",
    "#  \n",
    "\n",
    "![](Image/image8.jpg)\n",
    "![](Image/image9.jpg)\n",
    "\n",
    "* **Back Propagation (Chain rule)**\n",
    "\n",
    " 각각의 X값이 최종 Y값에 어떤 영향을 미치는 지를 구하는데 이용한다. 먼저 Y가 가장 가까운 Layer에서는 연산식을 알기 때문에 미분한 값을 구할 수 있으며, 그다음 가까운 Layer에서도 인접한 미분값을 구할 수 있는데, 이를 곱해주기만 하면 (Chain rule) 각각의 Weight들을 학습할 수 있다.\n",
    " \n",
    "* **Vanishing gradient**\n",
    "\n",
    " Layer가 매우 깊어지면 Back-Propagation을 하면서 Weight값이 사라지는 현상이다. 그 이유는 편미분한 값을 구할 때 sigmoid 함수로 0 ~ 1 사이의 값을 만드는데, 이를 반복하여 곱하면서 우연히 0에 가까운 값이 나오면 급격하게 0에 가까워지는 현상이 발생한다. 이를 보완하기 위해 Activation Function을 바꾸는데 단순하게 ReLU 함수를 사용하면 훨씬 잘 동작함을 알 수 있다. 이 외에도 여러가지 Activation Function들이 나오고 있다.\n",
    " \n",
    "* **Set the initial weight**\n",
    " \n",
    " Weight값을 랜덤하게 셋팅하는 것이 아니라 최대한 결과 데이터에 가깝도록 셋팅하자는 이슈이다. 이 이슈는 아직 해결되지 않고 계속 발전중인 분야이다. 하지만 지금도 어느정도 잘 동작하는 방법이 존재한다. 2006년에 RBM 이라는 모든 레이어의 인접한 2개 Layer 끼리만 학습하면서 각각의 Weight를 학습하는 아이디어인데 학습효과가 좋아서 실제 모두 Learning하는 것을 튜닝이라고 부를 정도이다. 하지만 구현방법이 조금 까다롭다.\n",
    " \n",
    "* **Xavier / He**\n",
    " \n",
    " RBM을 이용하지 않고 아주 단순한 방법으로도 효과가 좋은 방법이 있다. Fanin/out 값을 이용한 것인데 왜 동작하는지는 모르지만 구현하기 매우 쉽고, 학습효과도 매우 좋다. Xavier initialization, He의 방법을 이용해보자.\n",
    "  \n",
    "* **Dropout**\n",
    " \n",
    " Deep Learning 에서 너무 Deep/Wide 하게 Layer를 두면 Overfitting 되어 테스트 데이터에서는 잘 예측하지 못하는 현상이 발생하는데, 이 때 쓸 수 있는 방법으로 임의의 노드들을 학습하지 않도록 참여시키지 않는 것이다. 단, 최종적으로 예측모델을 사용할 때는 모든 노드를 사용한다. 아직 이유는 밝혀지지 않았지만 예측률이 좋아진다.\n",
    " \n",
    "* **Ensemble**\n",
    " \n",
    " 똑같은 형태의 모델들을 여러개 만들어 학습시키면 각각의 Weight가 다른데, 각각의 모델의 결과를 합산하여 좀 더 성능향상을 꾀하는 방법이다.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1 XOR Problem (Linear Regression, Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 3-1 XOR Problem with Linear Regression, Neural Network.\n",
      "# 1. Linear Regression. Learning Started..\n",
      "Step:    0, Cost: 0.72522, Accuracy: 75.00\n",
      "Step: 2000, Cost: 0.69315, Accuracy: 50.00\n",
      "Step: 4000, Cost: 0.69315, Accuracy: 50.00\n",
      "Step: 6000, Cost: 0.69315, Accuracy: 50.00\n",
      "Step: 8000, Cost: 0.69315, Accuracy: 50.00\n",
      "Step: 10000, Cost: 0.69315, Accuracy: 50.00\n",
      "\n",
      "# 2. Neural Network with 2 Layers. Learning Started..\n",
      "Step:    0, Cost: 0.72944, Accuracy: 50.00\n",
      "Step: 2000, Cost: 0.14233, Accuracy: 100.00\n",
      "Step: 4000, Cost: 0.02958, Accuracy: 100.00\n",
      "Step: 6000, Cost: 0.01435, Accuracy: 100.00\n",
      "Step: 8000, Cost: 0.00913, Accuracy: 100.00\n",
      "Step: 10000, Cost: 0.00660, Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "# 3-1 XOR Problem with Linear Regression, Neural Network\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "## 1. Linear Regression\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) * 100\n",
    "\n",
    "print(\"# 3-1 XOR Problem with Linear Regression, Neural Network.\")\n",
    "print(\"# 1. Linear Regression. Learning Started..\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, a, _ = sess.run([cost, accuracy, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: %4d, Cost: %.5f, Accuracy: %.2f\"% (step, cost_val, a))\n",
    "            \n",
    "## 2. Neural Network with 2 Layers\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), name=\"bias2\")\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) * 100\n",
    "    \n",
    "print(\"\\n# 2. Neural Network with 2 Layers. Learning Started..\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, a, _ = sess.run([cost, accuracy, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: %4d, Cost: %.5f, Accuracy: %.2f\"% (step, cost_val, a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NetworkModel' object has no attribute 'cost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-22ee96f9a6c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mcost_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step: %4d, Cost: %.5f, Accuracy: %.2f\"\u001b[0m\u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-22ee96f9a6c5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_data, y_data)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NetworkModel' object has no attribute 'cost'"
     ]
    }
   ],
   "source": [
    "# 3-1 XOR Problem with Linear Regression, Neural Network\n",
    "\n",
    "# 파이썬 클래스 공부한다음 다시 작성하자..\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "class NetworkModel:\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, [None, 2])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # 1. Linear Regression\n",
    "    def _build_linear_regression(self):\n",
    "        tf.set_random_seed(777)  # for reproducibility\n",
    "        \n",
    "        with tf.variable_scope(self.name):\n",
    "            W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "            b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "            # Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "            hypothesis = tf.sigmoid(tf.matmul(self.X, W) + b)\n",
    "            # cost/loss function\n",
    "            cost = -tf.reduce_mean(self.Y * tf.log(hypothesis) + (1 - self.Y) * tf.log(1 - hypothesis))\n",
    "            train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "            \n",
    "            # Accuracy computation\n",
    "            # True if hypothesis>0.5 else False\n",
    "            predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, self.Y), dtype=tf.float32)) * 100\n",
    "    \n",
    "    def train(self, x_data, y_data):\n",
    "        return self.sess.run([self.cost, self.train], feed_dict={self.X: x_data, self.Y: y_data})\n",
    "    \n",
    "    def get_accuracy(self, x_test, y_test):\n",
    "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test})\n",
    "\n",
    " # 1. Linear Regression\n",
    "sess1 = tf.Session()\n",
    "linear = NetworkModel(sess1, \"LinearRegression\")\n",
    "linear._build_linear_regression()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = linear.train(x_data, y_data)\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: %4d, Cost: %.5f, Accuracy: %.2f\"% (step, cost_val, linear.get_accuracy(x_data, y_data)))\n",
    "            \n",
    "# 2. Neural Network with 2 Layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2 XOR with Deep-Wide Learning (Vanishing gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 3-2 XOR with Deep + Wide Learing (Vanish gradient). Learning Started..\n",
      "Step:    0, Cost: 1.78570, Accuracy: 50.00\n",
      "Step: 2000, Cost: 0.69259, Accuracy: 50.00\n",
      "Step: 4000, Cost: 0.68282, Accuracy: 75.00\n",
      "Step: 6000, Cost: 0.00730, Accuracy: 100.00\n",
      "Step: 8000, Cost: 0.00258, Accuracy: 100.00\n",
      "Step: 10000, Cost: 0.00153, Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "# 3-2 XOR with Deep + Wide Learing (Vanishing gradient)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "inputs = 15\n",
    "# 이거.. Input을 늘려서 더 Wide하게 하면.. 정확해지는데... Vanish는 Wide하게 하면 좋아지나..?\n",
    "# sigmoid는 오히려 잘 동작하고.. relu 쓰면 nan 나오는데요..? 이상하네\n",
    "\n",
    "# 10 Layer + 15 inputs every layers\n",
    "W1 = tf.Variable(tf.random_normal([2, inputs]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([inputs]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "# relu\n",
    "# layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([inputs]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([inputs]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([inputs]), name='bias4')\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "W5 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight5')\n",
    "b5 = tf.Variable(tf.random_normal([inputs]), name='bias5')\n",
    "layer5 = tf.sigmoid(tf.matmul(layer4, W5) + b5)\n",
    "\n",
    "W6 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight6')\n",
    "b6 = tf.Variable(tf.random_normal([inputs]), name='bias6')\n",
    "layer6 = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "W7 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight7')\n",
    "b7 = tf.Variable(tf.random_normal([inputs]), name='bias7')\n",
    "layer7 = tf.sigmoid(tf.matmul(layer6, W7) + b7)\n",
    "\n",
    "W8 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight8')\n",
    "b8 = tf.Variable(tf.random_normal([inputs]), name='bias8')\n",
    "layer8 = tf.sigmoid(tf.matmul(layer7, W8) + b8)\n",
    "\n",
    "W9 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight9')\n",
    "b9 = tf.Variable(tf.random_normal([inputs]), name='bias9')\n",
    "layer9 = tf.sigmoid(tf.matmul(layer8, W9) + b9)\n",
    "\n",
    "W10 = tf.Variable(tf.random_normal([inputs, 1]), name='weight10')\n",
    "b10 = tf.Variable(tf.random_normal([1]), name='bias10')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer9, W10) + b10)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) * 100\n",
    "    \n",
    "print(\"# 3-2 XOR with Deep + Wide Learing (Vanish gradient). Learning Started..\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, a, _ = sess.run([cost, accuracy, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: %4d, Cost: %.5f, Accuracy: %.2f\"% (step, cost_val, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Bonus) 실제 데이터를 사용하기 전 알아야할 개념\n",
    "\n",
    "![](Image/image10.jpg)\n",
    "\n",
    "* **Learning Rate**\n",
    "\n",
    "  Gradient decent에서 변경하는 값의 비율로 너무 크면 NaN으로 값이 튀거나 너무 작으면 Local Minimum에 빠져 Cost가 최소화되지 않는 현상이 발생할 수 있다.\n",
    "\n",
    "* **Data preprocessing (Regularization)**\n",
    "\n",
    "  데이터가 너무 편차가 크게 차이가 나는 경우에는 학습이 제대로 되지 않을 수 있는데 이때는 데이터를 정규분포화한다.\n",
    "  \n",
    "* **Training Set / Test Set**\n",
    "\n",
    "  데이터를 학습할 때는 반드시 학습데이터와 일정비율(10~20%)은 테스트 데이터로 분리하여야 모델이 제대로 동작하는지 평가할 수 있다. Validation Set을 한번 더 나눠 학습데이터를 튜닝할 때 이용하기도 한다. 또한 데이터가 너무 많을 경우에 메모리에 올릴 수 없는 상황에서는 Online learning으로 쪼개서 학습하도록 하고, epoch과 batch_size를 결정하여 데이터를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (실습1) 다양한 모델을 이용한 MNIST 예측하기\n",
    "\n",
    " > 이번 실습에서는 1~3 챕터에서 배웠던 내용을 숫자 0~9까지 손글씨로 입력한 이미지 데이터를 실제로 어떤 값을 나타내는지 예측하는 모델을 만들어보고자 한다. 여러가지 모델을 적용하여 최대한 예측률을 높여보자.\n",
    "\n",
    "![](Image/image11.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels(one-hot) :  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "number :  7\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Tensorflow에 있는 MNIST 데이터를 이용하였음.\n",
    "# image는 784 (28*28)개의 0.0~1.0 사이의 값으로 표현된 이미지\n",
    "# label은 10개 (0~9) 까지의 class로 one_hot 데이터 형식\n",
    "# train은 55000개, test는 10000개\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(np.shape(mnist.train.images))\n",
    "print(np.shape(mnist.train.labels))\n",
    "print(np.shape(mnist.test.images))\n",
    "print(np.shape(mnist.test.labels))\n",
    "\n",
    "# 데이터 구경하기\n",
    "plt.imshow(mnist.test.images[0].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n",
    "print(\"labels(one-hot) : \", mnist.test.labels[0])\n",
    "print(\"number : \", np.argmax(mnist.test.labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습1 에 공통된 코드인 러닝하는 함수를 따로 분리하였으니, 아래 부분을 코드를 최초로 실행시킨 후에\n",
    "모델만 만들어서 코드를 실행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-c86e8663498a>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "learningRate = 0.01\n",
    "trainingEpochs = 10\n",
    "batchSize = 100\n",
    "\n",
    "def set_hyper_parameters(learning_rate, training_epochs, batch_size):\n",
    "    global learningRate\n",
    "    global trainingEpochs\n",
    "    global batchSize\n",
    "    learningRate = learning_rate\n",
    "    trainingEpochs = training_epochs\n",
    "    batchSize = batch_size\n",
    "\n",
    "def get_accuracy(sess, x_data, y_data):\n",
    "    return sess.run(accuracy, feedict = {X: x_data, Y: y_data})\n",
    "\n",
    "def run_train_model(sess):\n",
    "    print(\"===== Learning Started... =====\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(trainingEpochs + 1):\n",
    "        total_batch = int(mnist.train.num_examples / batchSize)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batchSize)\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "        train_cost, train_acc = sess.run([cost, accuracy], feed_dict={X: mnist.train.images, Y: mnist.train.labels})\n",
    "        test_cost, test_acc = sess.run([cost, accuracy], feed_dict={X: mnist.test.images, Y: mnist.test.labels})\n",
    "\n",
    "        if((epoch % (trainingEpochs/5)) == 0):\n",
    "            print(\"[{0} epoch, {1:0.2f}s pass] [TEST_DATA] Cost: {2:0.5f} , Accuracy: {3:0.3f} %\"\n",
    "            .format(epoch, (time.time()-start_time), test_cost, test_acc * 100))\n",
    "    \n",
    "    train_cost, train_acc = sess.run([cost, accuracy], feed_dict={X: mnist.train.images, Y: mnist.train.labels})\n",
    "    test_cost, test_acc = sess.run([cost, accuracy], feed_dict={X: mnist.test.images, Y: mnist.test.labels})\n",
    "\n",
    "    print(\"===== Learning Finished... =====\")\n",
    "    print(\"[Hyper Parameters] Learing Rate: %f, epoch: %d, batch_size: %d\" % (learningRate, trainingEpochs, batchSize))\n",
    "    print(\"[Accuracy gap: %.3f per, Learning time: %.2fs]\" % (((train_acc-test_acc) * 100), (time.time()-start_time)))\n",
    "    print(\"[TRAIN_DATA] Cost: {0:0.5f} , Accuracy: {1:0.3f} %\".format(train_cost, train_acc * 100))\n",
    "    print(\"[TEST_DATA] Cost: {0:0.5f} , Accuracy: {1:0.3f} %\".format(test_cost, test_acc * 100))\n",
    "\n",
    "# [주의사항] \n",
    "# Evaluation def 을 사용하기 위해서는 다음과 같은 Tensor를 모두 정의해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습1-1) Logistic Classification (Regression)\n",
    "이건 동작을 안하네..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "=== Learning Started ===\n",
      "step : 0000, cost: nan, Accuracy: 90.000 %\n",
      "step : 0001, cost: nan, Accuracy: 90.000 %\n",
      "step : 0002, cost: nan, Accuracy: 90.000 %\n",
      "step : 0003, cost: nan, Accuracy: 90.000 %\n",
      "step : 0004, cost: nan, Accuracy: 90.000 %\n",
      "step : 0005, cost: nan, Accuracy: 90.000 %\n",
      "step : 0006, cost: nan, Accuracy: 90.000 %\n",
      "step : 0007, cost: nan, Accuracy: 90.000 %\n",
      "step : 0008, cost: nan, Accuracy: 90.000 %\n",
      "step : 0009, cost: nan, Accuracy: 90.000 %\n",
      "step : 0010, cost: nan, Accuracy: 90.000 %\n",
      "step : 0011, cost: nan, Accuracy: 90.000 %\n",
      "step : 0012, cost: nan, Accuracy: 90.000 %\n",
      "step : 0013, cost: nan, Accuracy: 90.000 %\n",
      "step : 0014, cost: nan, Accuracy: 90.000 %\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train = mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "\n",
    "# 1. Logistic (regression) classification\n",
    "\n",
    "# Hyper Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "n_features = 784\n",
    "n_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_features, n_classes]))\n",
    "b = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "#hypothesis = tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))\n",
    "# Simplified cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# logits = tf.matmul(X,W) + b\n",
    "# cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_train) #label must be one-hot\n",
    "# cost = tf.reduce_mean(cost_i)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) * 100\n",
    "\n",
    "#launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"=== Learning Started ===\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        current_cost, acc = sess.run([cost, accuracy], feed_dict={X: X_train, Y: Y_train})\n",
    "        print('step : %04d, cost: %.5f, Accuracy: %.3f' % (epoch, current_cost, acc), \"%\")\n",
    "            \n",
    "    print('Learning finished.')\n",
    "    # Test the model using test sets\n",
    "    #print(\"Accuracy: %.3f \" % accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습1-2) Logistic Classification (Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Learning Stated... =====\n",
      "[0 epoch, 0.62s pass] [TEST_DATA] Cost: 1.24081 , Accuracy: 74.940 %\n",
      "[10 epoch, 4.63s pass] [TEST_DATA] Cost: 0.49364 , Accuracy: 88.260 %\n",
      "[20 epoch, 8.62s pass] [TEST_DATA] Cost: 0.41648 , Accuracy: 89.870 %\n",
      "[30 epoch, 12.54s pass] [TEST_DATA] Cost: 0.38182 , Accuracy: 90.370 %\n",
      "[40 epoch, 16.58s pass] [TEST_DATA] Cost: 0.35793 , Accuracy: 90.720 %\n",
      "[50 epoch, 20.59s pass] [TEST_DATA] Cost: 0.34203 , Accuracy: 91.000 %\n",
      "===== Learning Finished... =====\n",
      "[Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
      "[Accuracy gap: 0.369 per, Learning time: 20.65s]\n",
      "[TRAIN_DATA] Cost: 0.32351 , Accuracy: 91.369 %\n",
      "[TEST_DATA] Cost: 0.34203 , Accuracy: 91.000 %\n"
     ]
    }
   ],
   "source": [
    "# 실습 1-2) Logistic classification (Softmax classfier)\n",
    "n_features = 784\n",
    "n_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "W = tf.Variable(tf.random_normal([n_features, n_classes]))\n",
    "b = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "set_hyper_parameters(learning_rate = 0.1, training_epochs = 50, batch_size = 100)\n",
    "\n",
    "# case 1. Cross entropy\n",
    "#logits = tf.matmul(X,W) + b\n",
    "#hypothesis = tf.nn.softmax(logits)\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) \n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# case 2. fancy Softmax by logits\n",
    "logits = tf.matmul(X,W) + b\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learningRate).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run with def\n",
    "run_train_model(sess)\n",
    "\n",
    "# ## Epoch / Batch_Size 변경\n",
    "\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 15, batch_size: 100\n",
    "# [Accuracy gap: -0.485 per, Learning time: 6.75s]\n",
    "# [TRAIN_DATA] Cost: 0.45354 , Accuracy: 88.955 %\n",
    "# [TEST_DATA] Cost: 0.44991 , Accuracy: 89.440 %\n",
    "\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 0.499 per, Learning time: 20.62s]\n",
    "# [TRAIN_DATA] Cost: 0.32619 , Accuracy: 91.429 %\n",
    "# [TEST_DATA] Cost: 0.35714 , Accuracy: 90.930 %\n",
    "\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 500\n",
    "# [Accuracy gap: -0.337 per, Learning time: 16.11s]\n",
    "# [TRAIN_DATA] Cost: 0.51811 , Accuracy: 87.973 %\n",
    "# [TEST_DATA] Cost: 0.51571 , Accuracy: 88.310 %\n",
    "\n",
    "\n",
    "# ## Learning Rate 변경\n",
    "\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 0.62s pass] [TEST_DATA] Cost: 5.48238 , Accuracy: 29.110 %\n",
    "# [10 epoch, 4.62s pass] [TEST_DATA] Cost: 1.08241 , Accuracy: 77.560 %\n",
    "# [20 epoch, 8.62s pass] [TEST_DATA] Cost: 0.82483 , Accuracy: 82.290 %\n",
    "# [30 epoch, 12.60s pass] [TEST_DATA] Cost: 0.71857 , Accuracy: 84.290 %\n",
    "# [40 epoch, 16.63s pass] [TEST_DATA] Cost: 0.65594 , Accuracy: 85.490 %\n",
    "# [50 epoch, 20.64s pass] [TEST_DATA] Cost: 0.61241 , Accuracy: 86.260 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.010000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: -0.555 per, Learning time: 20.70]\n",
    "# [TRAIN_DATA] Cost: 0.63007 , Accuracy: 85.705 %\n",
    "# [TEST_DATA] Cost: 0.61241 , Accuracy: 86.260 %\n",
    "\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 0.62s pass] [TEST_DATA] Cost: 1.24081 , Accuracy: 74.940 %\n",
    "# [10 epoch, 4.63s pass] [TEST_DATA] Cost: 0.49364 , Accuracy: 88.260 %\n",
    "# [20 epoch, 8.62s pass] [TEST_DATA] Cost: 0.41648 , Accuracy: 89.870 %\n",
    "# [30 epoch, 12.54s pass] [TEST_DATA] Cost: 0.38182 , Accuracy: 90.370 %\n",
    "# [40 epoch, 16.58s pass] [TEST_DATA] Cost: 0.35793 , Accuracy: 90.720 %\n",
    "# [50 epoch, 20.59s pass] [TEST_DATA] Cost: 0.34203 , Accuracy: 91.000 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 0.369 per, Learning time: 20.65s]\n",
    "# [TRAIN_DATA] Cost: 0.32351 , Accuracy: 91.369 %\n",
    "# [TEST_DATA] Cost: 0.34203 , Accuracy: 91.000 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습1-3) Deep + Wide NN\n",
    "Deep + Wide NN (5 Layers + 128 hide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Learning Stated... =====\n",
      "[0 epoch, 1.31s pass] [TEST_DATA] Cost: 1.01420 , Accuracy: 66.800 %\n",
      "[10 epoch, 12.28s pass] [TEST_DATA] Cost: 0.38927 , Accuracy: 87.810 %\n",
      "[20 epoch, 23.17s pass] [TEST_DATA] Cost: 0.32992 , Accuracy: 89.900 %\n",
      "[30 epoch, 33.78s pass] [TEST_DATA] Cost: 0.29821 , Accuracy: 91.060 %\n",
      "[40 epoch, 44.54s pass] [TEST_DATA] Cost: 0.28224 , Accuracy: 91.620 %\n",
      "[50 epoch, 55.63s pass] [TEST_DATA] Cost: 0.27328 , Accuracy: 92.090 %\n",
      "===== Learning Finished... =====\n",
      "[Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
      "[Accuracy gap: 4.721 per, Learning time: 55.82s]\n",
      "[TRAIN_DATA] Cost: 0.11593 , Accuracy: 96.811 %\n",
      "[TEST_DATA] Cost: 0.27328 , Accuracy: 92.090 %\n"
     ]
    }
   ],
   "source": [
    "n_features = 784\n",
    "n_classes = 10\n",
    "hide = 128\n",
    "\n",
    "set_hyper_parameters(learning_rate = 0.1, training_epochs = 50, batch_size = 100)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_features, hide]), name='weight1')\n",
    "b = tf.Variable(tf.random_normal([hide]), name='bias1')\n",
    "\n",
    "H_W1 = tf.Variable(tf.random_normal([hide, hide]), name='weight2')\n",
    "H_b1 = tf.Variable(tf.random_normal([hide]), name='bias2')\n",
    "\n",
    "H_W2 = tf.Variable(tf.random_normal([hide, hide]), name='weight3')\n",
    "H_b2 = tf.Variable(tf.random_normal([hide]), name='bias3')\n",
    "\n",
    "H_W3 = tf.Variable(tf.random_normal([hide, hide]), name='weight4')\n",
    "H_b3 = tf.Variable(tf.random_normal([hide]), name='bias4')\n",
    "\n",
    "last_W = tf.Variable(tf.random_normal([hide, n_classes]), name='weight5')\n",
    "last_b = tf.Variable(tf.random_normal([n_classes]), name='bias5')\n",
    "\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, H_W1) + H_b1)\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, H_W2) + H_b2)\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3, H_W3) + H_b3)\n",
    "\n",
    "# fancy Softmax by logits\n",
    "logits = tf.matmul(layer4, last_W) + last_b\n",
    "\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learningRate).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run with def\n",
    "run_train_model(sess)\n",
    "\n",
    "# # 1.3 기본\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 1.38s pass] [TEST_DATA] Cost: 0.92668 , Accuracy: 69.830 %\n",
    "# [10 epoch, 12.39s pass] [TEST_DATA] Cost: 0.37498 , Accuracy: 88.140 %\n",
    "# [20 epoch, 23.23s pass] [TEST_DATA] Cost: 0.31178 , Accuracy: 90.270 %\n",
    "# [30 epoch, 34.02s pass] [TEST_DATA] Cost: 0.28405 , Accuracy: 91.190 %\n",
    "# [40 epoch, 44.73s pass] [TEST_DATA] Cost: 0.26554 , Accuracy: 91.880 %\n",
    "# [50 epoch, 55.55s pass] [TEST_DATA] Cost: 0.25685 , Accuracy: 92.380 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 4.527 per, Learning time: 55.74s]\n",
    "# [TRAIN_DATA] Cost: 0.11009 , Accuracy: 96.907 %\n",
    "# [TEST_DATA] Cost: 0.25685 , Accuracy: 92.380 %\n",
    "\n",
    "# # 1.3 + AdamOptimizer\n",
    "\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 1.26s pass] [TEST_DATA] Cost: 0.68084 , Accuracy: 77.960 %\n",
    "# [10 epoch, 12.62s pass] [TEST_DATA] Cost: 0.28464 , Accuracy: 91.250 %\n",
    "# [20 epoch, 23.67s pass] [TEST_DATA] Cost: 0.24727 , Accuracy: 92.730 %\n",
    "# [30 epoch, 34.70s pass] [TEST_DATA] Cost: 0.24068 , Accuracy: 93.110 %\n",
    "# [40 epoch, 45.59s pass] [TEST_DATA] Cost: 0.24324 , Accuracy: 93.570 %\n",
    "# [50 epoch, 56.54s pass] [TEST_DATA] Cost: 0.25110 , Accuracy: 93.630 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 5.706 per, Learning time: 56.73s]\n",
    "# [TRAIN_DATA] Cost: 0.03375 , Accuracy: 99.336 %\n",
    "# [TEST_DATA] Cost: 0.25110 , Accuracy: 93.630 %\n",
    "\n",
    "\n",
    "# # 1.3 ReLU + Xavier (ReLU, Xavier를 따로 하면 학습이 안되네..?)\n",
    "\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 1.19s pass] [TEST_DATA] Cost: 0.62437 , Accuracy: 82.610 %\n",
    "# [10 epoch, 11.57s pass] [TEST_DATA] Cost: 0.18562 , Accuracy: 94.560 %\n",
    "# [20 epoch, 22.19s pass] [TEST_DATA] Cost: 0.13110 , Accuracy: 96.270 %\n",
    "# [30 epoch, 32.63s pass] [TEST_DATA] Cost: 0.10670 , Accuracy: 96.940 %\n",
    "# [40 epoch, 43.46s pass] [TEST_DATA] Cost: 0.09446 , Accuracy: 97.150 %\n",
    "# [50 epoch, 54.02s pass] [TEST_DATA] Cost: 0.08862 , Accuracy: 97.360 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.010000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 1.511 per, Learning time: 54.21s]\n",
    "# [TRAIN_DATA] Cost: 0.04265 , Accuracy: 98.871 %\n",
    "# [TEST_DATA] Cost: 0.08862 , Accuracy: 97.360 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습1-4) ReLU, Xavier, Dropout, AdamOptimizer\n",
    "ReLU + Xavier + Dropout 0.7 + AdamOptimizer를 # 1.3 과 동일한 모델에 적용해본 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Learning Stated... =====\n",
      "[0 epoch, 2.09s pass] [TEST_DATA] Cost: 0.32104 , Accuracy: 90.930 %\n",
      "[10 epoch, 22.67s pass] [TEST_DATA] Cost: 0.17034 , Accuracy: 95.320 %\n",
      "[20 epoch, 41.70s pass] [TEST_DATA] Cost: 0.17134 , Accuracy: 95.750 %\n",
      "[30 epoch, 60.58s pass] [TEST_DATA] Cost: 0.15388 , Accuracy: 96.290 %\n",
      "[40 epoch, 79.49s pass] [TEST_DATA] Cost: 0.16610 , Accuracy: 96.340 %\n",
      "[50 epoch, 98.45s pass] [TEST_DATA] Cost: 0.19230 , Accuracy: 96.250 %\n",
      "===== Learning Finished... =====\n",
      "[Hyper Parameters] Learing Rate: 0.002000, epoch: 50, batch_size: 100\n",
      "[Accuracy gap: 1.546 per, Learning time: 98.80s]\n",
      "[TRAIN_DATA] Cost: 0.07849 , Accuracy: 97.816 %\n",
      "[TEST_DATA] Cost: 0.18003 , Accuracy: 96.270 %\n"
     ]
    }
   ],
   "source": [
    "n_features = 784\n",
    "n_classes = 10\n",
    "hide = 128\n",
    "keep_prob = 0.7\n",
    "\n",
    "set_hyper_parameters(learning_rate = 0.002, training_epochs = 50, batch_size = 100)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "W = tf.get_variable('weight1', shape = [n_features, hide], \n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([hide]), name='bias1')\n",
    "\n",
    "H_W1 = tf.get_variable('weight2', shape = [hide, hide], \n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "H_b1 = tf.Variable(tf.random_normal([hide]), name='bias2')\n",
    "\n",
    "H_W2 = tf.get_variable('weight3', shape = [hide, hide],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "H_b2 = tf.Variable(tf.random_normal([hide]), name='bias3')\n",
    "\n",
    "H_W3 = tf.get_variable('weight4', shape = [hide, hide],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "H_b3 = tf.Variable(tf.random_normal([hide]), name='bias4')\n",
    "\n",
    "last_W = tf.get_variable('weight5', shape = [hide, n_classes],\n",
    "                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "last_b = tf.Variable(tf.random_normal([n_classes]), name='bias5')\n",
    "\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W) + b)\n",
    "layer1 = tf.nn.dropout(layer1, keep_prob=keep_prob)\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, H_W1) + H_b1)\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob=keep_prob)\n",
    "layer3 = tf.nn.relu(tf.matmul(layer2, H_W2) + H_b2)\n",
    "layer3 = tf.nn.dropout(layer3, keep_prob=keep_prob)\n",
    "layer4 = tf.nn.relu(tf.matmul(layer3, H_W3) + H_b3)\n",
    "layer4 = tf.nn.dropout(layer4, keep_prob=keep_prob)\n",
    "\n",
    "# fancy Softmax by logits\n",
    "logits = tf.matmul(layer4, last_W) + last_b # dropout 1.0\n",
    "\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learningRate).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run with def\n",
    "run_train_model(sess)\n",
    "\n",
    "# # 1.4 ReLU + Xavier + AdamOptimizer\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 1.25s pass] [TEST_DATA] Cost: 0.16134 , Accuracy: 95.200 %\n",
    "# [10 epoch, 12.19s pass] [TEST_DATA] Cost: 0.11121 , Accuracy: 97.470 %\n",
    "# [20 epoch, 23.43s pass] [TEST_DATA] Cost: 0.12800 , Accuracy: 97.490 %\n",
    "# [30 epoch, 34.83s pass] [TEST_DATA] Cost: 0.12125 , Accuracy: 97.680 %\n",
    "# [40 epoch, 46.38s pass] [TEST_DATA] Cost: 0.13925 , Accuracy: 97.900 %\n",
    "# [50 epoch, 58.11s pass] [TEST_DATA] Cost: 0.14305 , Accuracy: 98.110 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.001000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 1.854 per, Learning time: 58.30s]\n",
    "# [TRAIN_DATA] Cost: 0.00121 , Accuracy: 99.964 %\n",
    "# [TEST_DATA] Cost: 0.14305 , Accuracy: 98.110 %\n",
    "\n",
    "\n",
    "# # 1.4 RELU + Xavier + AdamOptimizer + Dropout 0.7\n",
    "# # 여기서는 별로 안좋아졌는데.. 체감상 Overfitting되는 경우에 좋아질 것 같다.\n",
    "# # 지금 모델이 생각보다 Deep, Wide가 잘 맞는것으로 보임\n",
    "\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 2.09s pass] [TEST_DATA] Cost: 0.32104 , Accuracy: 90.930 %\n",
    "# [10 epoch, 22.67s pass] [TEST_DATA] Cost: 0.17034 , Accuracy: 95.320 %\n",
    "# [20 epoch, 41.70s pass] [TEST_DATA] Cost: 0.17134 , Accuracy: 95.750 %\n",
    "# [30 epoch, 60.58s pass] [TEST_DATA] Cost: 0.15388 , Accuracy: 96.290 %\n",
    "# [40 epoch, 79.49s pass] [TEST_DATA] Cost: 0.16610 , Accuracy: 96.340 %\n",
    "# [50 epoch, 98.45s pass] [TEST_DATA] Cost: 0.19230 , Accuracy: 96.250 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.002000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 1.546 per, Learning time: 98.80s]\n",
    "# [TRAIN_DATA] Cost: 0.07849 , Accuracy: 97.816 %\n",
    "# [TEST_DATA] Cost: 0.18003 , Accuracy: 96.270 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CNN (Convolutional N.N.)\n",
    "\n",
    "> CNN은 이미지 인식에 뛰어난 성능을 보이는 모델로 이미지의 특정 부분마다 고양이의 뇌가 활성화되는 것이 다르다는 아이디어에서 착안했다. 이미지를 부분부분 잘라서 영향도를 계산하고, 최종적으로는 Fully Connected Layer를 통해 객체가 무엇인지 분류할 수 있다.\n",
    "\n",
    "![](Image/image13.jpg)\n",
    "\n",
    "* Convolution Layer\n",
    "\n",
    " 전체이미지를 일정크기의 작은이미지를 Convolution한 값으로 새로운 필터이미지 N개를 생성한다. 이때 이미지 크기는 몇으로 할지, Stride(이미지를 건너뛰는 값)를 몇으로 할지 결정하면 Output의 크기가 결정된다. 각 필터에 사용된 Weight를 학습하여 실제로 최종 레이어에서 모든 Output을 기준으로 예측할 수 있다.\n",
    " \n",
    "* MAX Pooling\n",
    " \n",
    " Sampling 또는 Resizing으로 이미지 크기를 작게 만들어 학습시간 단축 시키는 효과 및 다양한 형태의 이미지를 인식하는 것을 목표로 한다.\n",
    " \n",
    "* Convolution Layer + Maxpooling Layer\n",
    "\n",
    " CNN에서는 이미지를 다양한 형태의 이미지로 재생성하고 이를 학습하여 각각의 Weight는 NN를 통해 구한다. 아직 어떤 모델로 구성하는 것이 효율적인지는 밝혀지지 않았지만 이런식으로 모델을 Deep하게 구성한 모델이 이미지 분류가 잘 동작한다.\n",
    " \n",
    "* Fully Connected Layer\n",
    "\n",
    " Conv, Maxpooling 등 필터를 통과한 후 각각의 수치들을 1차원 형식의 Array로 Flatten한 Layer를 통해 각각의 특징을 추출한 결과를 학습하여 결과를 도출한다. 이때 이 Fully Connected Layer도 Deep하게 학습할 수 있다. 결국 앞에서 여러가지 필터를 적용시킨 것은 특징을 도출한 것이고, 결국 각각의 요소에 대해 Deep Learning을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (실습2) CNN을 이용하여 MNIST 이미지 인식률 향상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Learning Started... =====\n",
      "[0 epoch, 47.13s pass] [TEST_DATA] Cost: 0.10575 , Accuracy: 96.840 %\n",
      "[3 epoch, 188.83s pass] [TEST_DATA] Cost: 0.05495 , Accuracy: 98.070 %\n",
      "[6 epoch, 329.00s pass] [TEST_DATA] Cost: 0.04381 , Accuracy: 98.490 %\n",
      "[9 epoch, 466.75s pass] [TEST_DATA] Cost: 0.03744 , Accuracy: 98.730 %\n",
      "[12 epoch, 607.46s pass] [TEST_DATA] Cost: 0.05556 , Accuracy: 98.330 %\n",
      "[15 epoch, 745.34s pass] [TEST_DATA] Cost: 0.04866 , Accuracy: 98.630 %\n",
      "===== Learning Finished... =====\n",
      "[Hyper Parameters] Learing Rate: 0.001000, epoch: 15, batch_size: 100\n",
      "[Accuracy gap: 0.808 per, Learning time: 764.45s]\n",
      "[TRAIN_DATA] Cost: 0.01563 , Accuracy: 99.438 %\n",
      "[TEST_DATA] Cost: 0.04866 , Accuracy: 98.630 %\n"
     ]
    }
   ],
   "source": [
    "# Lab 11 MNIST and Convolutional Neural Network\n",
    "\n",
    "# hyper parameters\n",
    "set_hyper_parameters(learning_rate = 0.001, training_epochs = 15, batch_size = 100)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
    "\n",
    "# Final FC 7x7x64 inputs -> 10 outputs\n",
    "W3 = tf.get_variable(\"W3\", shape=[7 * 7 * 64, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L2_flat, W3) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "# Run with def\n",
    "run_train_model(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RNN (Recurrent N.N.)\n",
    "\n",
    "![](Image/image14.jpg)\n",
    "\n",
    "* Sequence data\n",
    "\n",
    " 이전 상태의 데이터가 현재 상태에 영향을 미치는 연속적인 데이터 형태에서\n",
    " 잘 동작하는 모델에 대해서 알아보고자 한다.\n",
    "\n",
    "* Recurrent Neural Network (RNN)\n",
    "\n",
    " 이전 상태의 값 Ht-1과 현재의 입력값 Xt 로 새로운 상태의값 Ht를 만들어내는\n",
    " 수식으로 이루어지며, tanh를 통과하여 -1~1 사이의 출력값을 만들어낸다.\n",
    "\n",
    "* Recuurent Networks offer a lot of flexibility\n",
    "\n",
    " RNN은 조금 변형하여 많은 다양한 분야에 활용가능한데 대표적으로 형태에 따라 분류해보면\n",
    " one to many (image captioning - 이미지에 대한 설명)\n",
    " many to one (Sentiment classification - 문장을 하나의 감정으로)\n",
    " many to many (Machine translation - 번역) 등이 있다.\n",
    "\n",
    "#  \n",
    "\n",
    "![](Image/image15.jpg)\n",
    "\n",
    "* Stacked RNN\n",
    "\n",
    " RNN 도 Layer를 Deep하게 층을 쌓아서 학습가능하다.\n",
    "\n",
    "* Loss\n",
    "\n",
    " Loss function(수식은 정리안됨)은  sequence_loss 것을 이용하면 각각의 Output이 Y값과\n",
    " 얼마나 다른지를 Cost를 계산해준다. 이것을 똑같이 Minimize하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (실습3) RNN을 이용한 주가 예측\n",
    " 구글의 일일 주가 데이터 정보 (시작가, Low, High, Volume) 값으로 주가가 얼마에 종료되었는지 예측하는 것으로 실습을  해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-9e0626352b42>:77: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "[step: 0] loss: 214.6417236328125\n",
      "[step: 100] loss: 1.328886866569519\n",
      "[step: 200] loss: 1.1567217111587524\n",
      "[step: 300] loss: 1.0068023204803467\n",
      "[step: 400] loss: 0.8863711953163147\n",
      "[step: 500] loss: 0.8043670654296875\n",
      "RMSE: 0.05568068474531174\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "This script shows how to predict stock prices using a basic RNN\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    ''' Min Max Normalization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        input data to be normalized\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    data : numpy.ndarry\n",
    "        normalized data\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "\n",
    "    '''\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 500\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "xy = np.loadtxt('data-02-stock_daily.csv', delimiter=',')\n",
    "xy = xy[::-1]  # reverse order (chronically ordered)\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(xy) * 0.7)\n",
    "train_set = xy[0:train_size]\n",
    "test_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
    "\n",
    "# Scale each\n",
    "train_set = MinMaxScaler(train_set)\n",
    "test_set = MinMaxScaler(test_set)\n",
    "\n",
    "# build datasets\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series[i:i + seq_length, :]\n",
    "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "testX, testY = build_dataset(test_set, seq_length)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations + 1):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "\n",
    "        if(i % 100 == 0) : \n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(testY)\n",
    "    plt.plot(test_predict)\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
