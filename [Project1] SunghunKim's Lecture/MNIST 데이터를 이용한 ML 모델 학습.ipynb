{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-21665c0ead94>:14: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Tensorflow에 있는 MNIST 데이터를 이용하였음.\n",
    "# image는 784 (28*28)개의 0.0~1.0 사이의 값으로 표현된 이미지\n",
    "# label은 10개 (0~9) 까지의 class로 one_hot 데이터 형식\n",
    "# train은 55000개, test는 10000개\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(np.shape(mnist.train.images))\n",
    "print(np.shape(mnist.train.labels))\n",
    "print(np.shape(mnist.test.images))\n",
    "print(np.shape(mnist.test.labels))\n",
    "\n",
    "X_train = mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "nb_features = 784\n",
    "nb_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 구경하기\n",
    "plt.imshow(mnist.test.images[0:1].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic (regression) classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0000, cost: nan\n",
      "step : 0020, cost: nan\n",
      "step : 0040, cost: nan\n",
      "step : 0060, cost: nan\n",
      "step : 0080, cost: nan\n",
      "Learning finished.\n",
      "Accuracy: 0.900 \n",
      "WARNING:tensorflow:From <ipython-input-10-61d572d64e99>:40: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "Prediction :  Tensor(\"ArgMax:0\", shape=(1,), dtype=int64)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_4' with dtype float and shape [?,784]\n\t [[node Placeholder_4 (defined at <ipython-input-10-61d572d64e99>:4)  = Placeholder[dtype=DT_FLOAT, shape=[?,784], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_4', defined at:\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-61d572d64e99>\", line 4, in <module>\n    X = tf.placeholder(tf.float32, [None, nb_features])\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6251, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_4' with dtype float and shape [?,784]\n\t [[node Placeholder_4 (defined at <ipython-input-10-61d572d64e99>:4)  = Placeholder[dtype=DT_FLOAT, shape=[?,784], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_4' with dtype float and shape [?,784]\n\t [[{{node Placeholder_4}} = Placeholder[dtype=DT_FLOAT, shape=[?,784], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-61d572d64e99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Prediction : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_max\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Label : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_max\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Greys'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nearest'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_4' with dtype float and shape [?,784]\n\t [[node Placeholder_4 (defined at <ipython-input-10-61d572d64e99>:4)  = Placeholder[dtype=DT_FLOAT, shape=[?,784], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_4', defined at:\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-61d572d64e99>\", line 4, in <module>\n    X = tf.placeholder(tf.float32, [None, nb_features])\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6251, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Hyun-Jin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_4' with dtype float and shape [?,784]\n\t [[node Placeholder_4 (defined at <ipython-input-10-61d572d64e99>:4)  = Placeholder[dtype=DT_FLOAT, shape=[?,784], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# 이거 스터디 할 때는 문제없는데 왜 MNIST에 적용하면 문제가 되는거지?\n",
    "\n",
    "# 1. Logistic (regression) classification\n",
    "X = tf.placeholder(tf.float32, [None, nb_features])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([nb_features, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))\n",
    "#hypothesis =  tf.sigmoid(tf.matmul(X, W) + b)  \n",
    "hypothesis = tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "#launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        sess.run(optimizer, feed_dict={X: X_train, Y: Y_train})\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            current_cost = sess.run(cost, feed_dict={X: X_train, Y: Y_train})\n",
    "            print('step : %04d, cost: %.5f' % (i, current_cost))\n",
    "            \n",
    "    print('Learning finished.')\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: %.3f \" % accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    # Show random data\n",
    "    r = random.randint(0, mnist.test.num_examples-1)\n",
    "    print('Prediction : ', tf.arg_max(mnist.test.images[r:r+1], 1))\n",
    "    print('Label : ', sess.run(tf.arg_max(hypothesis, 1)))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic classification (Softmax classfier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0000, cost: 12.070902\n",
      "step : 0020, cost: 7.864077\n",
      "step : 0040, cost: 6.092525\n",
      "step : 0060, cost: 4.938337\n",
      "step : 0080, cost: 4.160623\n",
      "Learning finished.\n",
      "Accuracy:  0.4402\n"
     ]
    }
   ],
   "source": [
    "# 2. Logistic classification (Softmax classfier)\n",
    "X = tf.placeholder(tf.float32, [None, nb_features])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([nb_features, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# case 1. Cross entropy\n",
    "#logits = tf.matmul(X,W) + b\n",
    "#hypothesis = tf.nn.softmax(logits)\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) \n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# case 2. fancy Softmax by logits\n",
    "logits = tf.matmul(X,W) + b\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_train) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "#launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        sess.run(optimizer, feed_dict={X: X_train, Y: Y_train})\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            current_cost = sess.run(cost, feed_dict={X: X_train, Y: Y_train})\n",
    "            print('step : %04d, cost: %.5f' % (i, current_cost))\n",
    "            \n",
    "    print('Learning finished.')\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Softmax classifier Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2b19ff4b45e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 2.1 테스트를 위하여 기본 모델 셋팅\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnb_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# 2.1 테스트를 위하여 기본 모델 셋팅\n",
    "X = tf.placeholder(tf.float32, [None, nb_features])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([nb_features, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "#fancy Softmax by logits\n",
    "logits = tf.matmul(X,W) + b\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 step, 0.58s pass] cost: 15.98284 , accuracy: 11.15 %\n",
      "[100 step, 15.16s pass] cost: 3.53905 , accuracy: 44.92 %\n",
      "[200 step, 31.75s pass] cost: 2.19746 , accuracy: 60.83 %\n",
      "[300 step, 48.10s pass] cost: 1.71044 , accuracy: 67.72 %\n",
      "[400 step, 64.52s pass] cost: 1.45689 , accuracy: 71.98 %\n",
      "[500 step, 80.67s pass] cost: 1.29827 , accuracy: 74.73 %\n",
      "---Report---\n",
      "Time: 80.75s , accuracy: 74.73 %\n"
     ]
    }
   ],
   "source": [
    "# Ver1 (기본 테스팅)\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(501):\n",
    "        sess.run(optimizer, feed_dict= {X : X_train, Y: Y_train}) # 한번에 모든 데이터 학습\n",
    "    \n",
    "        if(step % 100 == 0):\n",
    "            r_cost, r_accuracy = sess.run([cost, accuracy], feed_dict= {X: mnist.test.images, Y: mnist.test.labels})\n",
    "            print(\"[{0} step, {1:0.2f}s pass] cost: {2:0.5f} , accuracy: {3:0.2f} %\".format(step, (time.time()-start_time), r_cost, r_accuracy*100))\n",
    "    \n",
    "    result = sess.run(accuracy, feed_dict= {X: mnist.test.images, Y: mnist.test.labels})\n",
    "    print(\"---Report---\")\n",
    "    print(\"Time: {0:0.2f}s , accuracy: {1:0.2f} %\".format((time.time()-start_time), result*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, batch_size = [10, 100]\n",
      "[epoch 0, 1.79s pass] cost: 1.19406 , accuracy: 75.81 %\n",
      "[epoch 10, 11.02s pass] cost: 0.49858 , accuracy: 88.50 %\n",
      "== Learning finished ==\n",
      "epoch, batch_size = [10, 300]\n",
      "[epoch 0, 1.02s pass] cost: 2.23098 , accuracy: 61.37 %\n",
      "[epoch 10, 6.84s pass] cost: 0.69606 , accuracy: 85.09 %\n",
      "== Learning finished ==\n",
      "epoch, batch_size = [10, 500]\n",
      "[epoch 0, 1.09s pass] cost: 3.32122 , accuracy: 47.96 %\n",
      "[epoch 10, 7.32s pass] cost: 0.90019 , accuracy: 82.02 %\n",
      "== Learning finished ==\n",
      "epoch, batch_size = [30, 100]\n",
      "[epoch 0, 1.31s pass] cost: 1.18397 , accuracy: 76.42 %\n",
      "[epoch 10, 10.53s pass] cost: 0.49518 , accuracy: 88.56 %\n",
      "[epoch 20, 19.81s pass] cost: 0.41887 , accuracy: 89.60 %\n",
      "[epoch 30, 30.11s pass] cost: 0.38300 , accuracy: 90.25 %\n",
      "== Learning finished ==\n",
      "epoch, batch_size = [30, 300]\n",
      "[epoch 0, 1.08s pass] cost: 2.43496 , accuracy: 57.15 %\n",
      "[epoch 10, 7.14s pass] cost: 0.69485 , accuracy: 84.61 %\n",
      "[epoch 20, 14.25s pass] cost: 0.55857 , accuracy: 87.27 %\n",
      "[epoch 30, 21.31s pass] cost: 0.49891 , accuracy: 88.40 %\n",
      "== Learning finished ==\n",
      "epoch, batch_size = [30, 500]\n",
      "[epoch 0, 1.21s pass] cost: 3.15758 , accuracy: 48.26 %\n",
      "[epoch 10, 8.31s pass] cost: 0.85196 , accuracy: 82.52 %\n",
      "[epoch 20, 15.60s pass] cost: 0.67421 , accuracy: 85.74 %\n",
      "[epoch 30, 23.12s pass] cost: 0.58824 , accuracy: 87.05 %\n",
      "== Learning finished ==\n",
      "epoch, batch_size = [50, 100]\n",
      "[epoch 0, 1.66s pass] cost: 1.22616 , accuracy: 75.61 %\n",
      "[epoch 10, 12.36s pass] cost: 0.49214 , accuracy: 88.92 %\n",
      "[epoch 20, 22.15s pass] cost: 0.41885 , accuracy: 89.99 %\n",
      "[epoch 30, 33.07s pass] cost: 0.38361 , accuracy: 90.41 %\n",
      "[epoch 40, 43.83s pass] cost: 0.36342 , accuracy: 90.60 %\n",
      "[epoch 50, 55.34s pass] cost: 0.34915 , accuracy: 90.99 %\n",
      "== Learning finished ==\n",
      "epoch, batch_size = [50, 300]\n",
      "[epoch 0, 1.38s pass] cost: 2.08285 , accuracy: 61.29 %\n",
      "[epoch 10, 8.79s pass] cost: 0.69522 , accuracy: 84.57 %\n",
      "[epoch 20, 15.71s pass] cost: 0.56221 , accuracy: 87.03 %\n",
      "[epoch 30, 23.62s pass] cost: 0.50218 , accuracy: 88.01 %\n",
      "[epoch 40, 30.63s pass] cost: 0.46831 , accuracy: 88.56 %\n",
      "[epoch 50, 37.29s pass] cost: 0.44188 , accuracy: 89.17 %\n",
      "== Learning finished ==\n",
      "epoch, batch_size = [50, 500]\n",
      "[epoch 0, 1.46s pass] cost: 3.64785 , accuracy: 42.12 %\n",
      "[epoch 10, 8.77s pass] cost: 0.86059 , accuracy: 81.61 %\n",
      "[epoch 20, 15.65s pass] cost: 0.66663 , accuracy: 85.27 %\n",
      "[epoch 30, 22.29s pass] cost: 0.58556 , accuracy: 86.77 %\n",
      "[epoch 40, 29.68s pass] cost: 0.53643 , accuracy: 87.64 %\n",
      "[epoch 50, 37.07s pass] cost: 0.50417 , accuracy: 88.29 %\n",
      "== Learning finished ==\n"
     ]
    }
   ],
   "source": [
    "# Ver 2 (epoch, batch)\n",
    "for training_epochs in range(10, 60, 20):\n",
    "    for batch_size in range(100, 600, 200):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #launch the graph in a session\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            print('epoch, batch_size = [%02d, %02d]' % (training_epochs, batch_size))\n",
    "            for epoch in range(training_epochs + 1):\n",
    "                avg_cost = 0\n",
    "                total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "                for i in range(total_batch + 1):\n",
    "                    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                    sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                    avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                \n",
    "                if epoch % 10 == 0:\n",
    "                    r_cost, r_accuracy = sess.run([cost, accuracy], feed_dict= {X: mnist.test.images, Y: mnist.test.labels})\n",
    "                    print(\"[epoch {0}, {1:0.2f}s pass] cost: {2:0.5f} , accuracy: {3:0.2f} %\".format(epoch, (time.time()-start_time), r_cost, r_accuracy*100));    \n",
    "            print('== Learning finished ==')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 epoch, 1.86s pass] cost: 1.66309 , accuracy: 45.78 %\n",
      "[10 epoch, 20.51s pass] cost: 0.52851 , accuracy: 83.52 %\n",
      "[20 epoch, 40.72s pass] cost: 0.39451 , accuracy: 87.75 %\n",
      "[30 epoch, 60.45s pass] cost: 0.32786 , accuracy: 90.11 %\n",
      "[40 epoch, 82.47s pass] cost: 0.29087 , accuracy: 91.35 %\n",
      "[50 epoch, 103.14s pass] cost: 0.26887 , accuracy: 91.94 %\n",
      "---Report---\n",
      "Time: 103.20s , accuracy: 91.94 %\n"
     ]
    }
   ],
   "source": [
    "# Ver3 (Neural Net : 5 Layers + sigmoid)\n",
    "Inputs = 784\n",
    "classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, nb_features])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([nb_features, 30]), name='weight1')\n",
    "b = tf.Variable(tf.random_normal([30]), name='bias1')\n",
    "\n",
    "H_W1 = tf.Variable(tf.random_normal([30, 25]), name='weight2')\n",
    "H_b1 = tf.Variable(tf.random_normal([25]), name='bias2')\n",
    "\n",
    "H_W2 = tf.Variable(tf.random_normal([25, 20]), name='weight3')\n",
    "H_b2 = tf.Variable(tf.random_normal([20]), name='bias3')\n",
    "\n",
    "H_W3 = tf.Variable(tf.random_normal([20, 15]), name='weight4')\n",
    "H_b3 = tf.Variable(tf.random_normal([15]), name='bias4')\n",
    "\n",
    "last_W = tf.Variable(tf.random_normal([15, nb_classes]), name='weight5')\n",
    "last_b = tf.Variable(tf.random_normal([nb_classes]), name='bias5')\n",
    "\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, H_W1) + H_b1)\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, H_W2) + H_b2)\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3, H_W3) + H_b3)\n",
    "\n",
    "logits = tf.matmul(layer4, last_W) + last_b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy\n",
    "#cost = tf.reduce_mean(- tf.reduce_sum( Y * tf.log(hypothesis), axis=1 )) \n",
    "\n",
    "#fancy Softmax by logits\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "traing_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(traing_epochs+1):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict= {X : batch_x, Y: batch_y}) # 한번당 batch_size만큼 학습\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        if(epoch % 10 == 0):\n",
    "            r_cost, r_accuracy = sess.run([cost, accuracy], feed_dict= {X: mnist.test.images, Y: mnist.test.labels})\n",
    "            print(\"[{0} epoch, {1:0.2f}s pass] cost: {2:0.5f} , accuracy: {3:0.2f} %\".format(epoch, (time.time()-start_time), r_cost, r_accuracy*100))\n",
    "    \n",
    "    result = sess.run(accuracy, feed_dict= {X: mnist.test.images, Y: mnist.test.labels})\n",
    "    print(\"---Report---\")\n",
    "    print(\"Time: {0:0.2f}s , accuracy: {1:0.2f} %\".format((time.time()-start_time), result*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 epoch, 0.74s pass] cost: 2.15098 , accuracy: 19.38 %\n",
      "[10 epoch, 5.58s pass] cost: 0.44486 , accuracy: 87.95 %\n",
      "[20 epoch, 10.42s pass] cost: 0.25768 , accuracy: 93.93 %\n",
      "[30 epoch, 15.38s pass] cost: 0.23350 , accuracy: 94.86 %\n",
      "[40 epoch, 20.36s pass] cost: 0.22672 , accuracy: 95.42 %\n",
      "---Report---\n",
      "Time: 24.79s , accuracy: 95.36 %\n"
     ]
    }
   ],
   "source": [
    "# Ver4 (Neural Net : 5 Layers + relu, AdamOptimizer)\n",
    "\n",
    "# 50, 100 : Time: 24.79s , accuracy: 95.36 %\n",
    "\n",
    "#parameters\n",
    "learning_rate = 0.01\n",
    "traing_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "Inputs = 784\n",
    "classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, Inputs])\n",
    "Y = tf.placeholder(tf.float32, [None, classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([Inputs, 30]), name='weight1')\n",
    "b = tf.Variable(tf.random_normal([30]), name='bias1')\n",
    "\n",
    "H_W1 = tf.Variable(tf.random_normal([30, 25]), name='weight2')\n",
    "H_b1 = tf.Variable(tf.random_normal([25]), name='bias2')\n",
    "\n",
    "H_W2 = tf.Variable(tf.random_normal([25, 20]), name='weight3')\n",
    "H_b2 = tf.Variable(tf.random_normal([20]), name='bias3')\n",
    "\n",
    "H_W3 = tf.Variable(tf.random_normal([20, 15]), name='weight4')\n",
    "H_b3 = tf.Variable(tf.random_normal([15]), name='bias4')\n",
    "\n",
    "last_W = tf.Variable(tf.random_normal([15, classes]), name='weight5')\n",
    "last_b = tf.Variable(tf.random_normal([classes]), name='bias5')\n",
    "\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W) + b)\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, H_W1) + H_b1)\n",
    "layer3 = tf.nn.relu(tf.matmul(layer2, H_W2) + H_b2)\n",
    "layer4 = tf.nn.relu(tf.matmul(layer3, H_W3) + H_b3)\n",
    "\n",
    "logits = tf.matmul(layer4, last_W) + last_b\n",
    "\n",
    "#fancy Softmax by logits\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(traing_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict= {X : batch_x, Y: batch_y}) # 한번당 batch_size만큼 학습\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        if(epoch % 10 == 0):\n",
    "            r_cost, r_accuracy = sess.run([cost, accuracy], feed_dict= {X: mnist.test.images, Y: mnist.test.labels})\n",
    "            print(\"[{0} epoch, {1:0.2f}s pass] cost: {2:0.5f} , accuracy: {3:0.2f} %\".format(epoch, (time.time()-start_time), r_cost, r_accuracy*100))\n",
    "    \n",
    "    result = sess.run(accuracy, feed_dict= {X: mnist.test.images, Y: mnist.test.labels})\n",
    "    print(\"---Report---\")\n",
    "    print(\"Time: {0:0.2f}s , accuracy: {1:0.2f} %\".format((time.time()-start_time), result*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-e759c1885346>:40: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "[0 epoch, 5.80s pass] cost: 0.19258 , accuracy: 94.54 %\n",
      "[10 epoch, 60.10s pass] cost: 0.17463 , accuracy: 96.52 %\n",
      "[20 epoch, 113.67s pass] cost: 0.15517 , accuracy: 96.92 %\n",
      "---Report---\n",
      "Time: 164.33s , accuracy: 97.34 %\n"
     ]
    }
   ],
   "source": [
    "# Ver5 (Neural Net : 5 Layers + relu, AdamOptimizer + Xavier Initialize)\n",
    "\n",
    "# 30, 100 : Time: 45.87s , accuracy: 97.02 %\n",
    "\n",
    "#parameters\n",
    "learning_rate = 0.01\n",
    "traing_epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "Inputs = 784\n",
    "classes = 10\n",
    "hide = 256\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, Inputs])\n",
    "Y = tf.placeholder(tf.float32, [None, classes])\n",
    "\n",
    "W = tf.get_variable(\"weight1\", shape=[Inputs, hide], initializer=tf.contrib.layers.xavier_initializer());\n",
    "b = tf.Variable(tf.random_normal([hide]), name='bias1')\n",
    "\n",
    "H_W1 = tf.get_variable(\"weight2\", shape=[hide, hide], initializer=tf.contrib.layers.xavier_initializer());\n",
    "H_b1 = tf.Variable(tf.random_normal([hide]), name='bias2')\n",
    "\n",
    "H_W2 = tf.get_variable(\"weight3\", shape=[hide, hide], initializer=tf.contrib.layers.xavier_initializer());\n",
    "H_b2 = tf.Variable(tf.random_normal([hide]), name='bias3')\n",
    "\n",
    "H_W3 = tf.get_variable(\"weight4\", shape=[hide, hide], initializer=tf.contrib.layers.xavier_initializer());\n",
    "H_b3 = tf.Variable(tf.random_normal([hide]), name='bias4')\n",
    "\n",
    "last_W = tf.get_variable(\"weight5\", shape=[hide, classes], initializer=tf.contrib.layers.xavier_initializer());\n",
    "last_b = tf.Variable(tf.random_normal([classes]), name='bias5')\n",
    "\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W) + b)\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, H_W1) + H_b1)\n",
    "layer3 = tf.nn.relu(tf.matmul(layer2, H_W2) + H_b2)\n",
    "layer4 = tf.nn.relu(tf.matmul(layer3, H_W3) + H_b3)\n",
    "\n",
    "logits = tf.matmul(layer4, last_W) + last_b\n",
    "\n",
    "#fancy Softmax by logits\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(traing_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict= {X : batch_x, Y: batch_y}) # 한번당 batch_size만큼 학습\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        if(epoch % 10 == 0):\n",
    "            r_cost, r_accuracy = sess.run([cost, accuracy], feed_dict= {X: mnist.test.images, Y: mnist.test.labels})\n",
    "            print(\"[{0} epoch, {1:0.2f}s pass] cost: {2:0.5f} , accuracy: {3:0.2f} %\".format(epoch, (time.time()-start_time), r_cost, r_accuracy*100))\n",
    "    \n",
    "    result = sess.run(accuracy, feed_dict= {X: mnist.test.images, Y: mnist.test.labels})\n",
    "    print(\"---Report---\")\n",
    "    print(\"Time: {0:0.2f}s , accuracy: {1:0.2f} %\".format((time.time()-start_time), result*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-19b8757660c0>:46: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "[0 epoch, 20.01s pass] cost: 0.36709 , accuracy: 91.22 %\n",
      "[5 epoch, 111.90s pass] cost: 0.33593 , accuracy: 92.46 %\n",
      "[10 epoch, 205.12s pass] cost: 0.58201 , accuracy: 86.65 %\n",
      "[15 epoch, 297.73s pass] cost: 0.58889 , accuracy: 84.15 %\n",
      "---Report---\n",
      "Time: 298.85s , accuracy: 84.15 %\n"
     ]
    }
   ],
   "source": [
    "# Ver6 (Neural Net : 5 Layers + relu, AdamOptimizer + Xavier Initialize + dropout)\n",
    "\n",
    "#parameters\n",
    "learning_rate = 0.01\n",
    "traing_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "Inputs = 784\n",
    "classes = 10\n",
    "hide = 512\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, Inputs])\n",
    "Y = tf.placeholder(tf.float32, [None, classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep_prob) rate 0.7 on the training\n",
    "\n",
    "W = tf.get_variable(\"weight1\", shape=[Inputs, hide], initializer=tf.contrib.layers.xavier_initializer());\n",
    "b = tf.Variable(tf.random_normal([hide]), name='bias1')\n",
    "\n",
    "H_W1 = tf.get_variable(\"weight2\", shape=[hide, hide], initializer=tf.contrib.layers.xavier_initializer());\n",
    "H_b1 = tf.Variable(tf.random_normal([hide]), name='bias2')\n",
    "\n",
    "H_W2 = tf.get_variable(\"weight3\", shape=[hide, hide], initializer=tf.contrib.layers.xavier_initializer());\n",
    "H_b2 = tf.Variable(tf.random_normal([hide]), name='bias3')\n",
    "\n",
    "H_W3 = tf.get_variable(\"weight4\", shape=[hide, hide], initializer=tf.contrib.layers.xavier_initializer());\n",
    "H_b3 = tf.Variable(tf.random_normal([hide]), name='bias4')\n",
    "\n",
    "last_W = tf.get_variable(\"weight5\", shape=[hide, classes], initializer=tf.contrib.layers.xavier_initializer());\n",
    "last_b = tf.Variable(tf.random_normal([classes]), name='bias5')\n",
    "\n",
    "_layer1 = tf.nn.relu(tf.matmul(X, W) + b)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob=keep_prob)\n",
    "\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1, H_W1) + H_b1)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob=keep_prob)\n",
    "\n",
    "_layer3 = tf.nn.relu(tf.matmul(layer2, H_W2) + H_b2)\n",
    "layer3 = tf.nn.dropout(_layer3, keep_prob=keep_prob)\n",
    "\n",
    "_layer4 = tf.nn.relu(tf.matmul(layer3, H_W3) + H_b3)\n",
    "layer4 = tf.nn.dropout(_layer4, keep_prob=keep_prob)\n",
    "\n",
    "logits = tf.matmul(layer4, last_W) + last_b\n",
    "\n",
    "#fancy Softmax by logits\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "correct = tf.equal(tf.argmax(logits, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(traing_epochs+1):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict= {X : batch_x, Y: batch_y, keep_prob: 0.7}) # 한번당 batch_size만큼 학습\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        if(epoch % 5 == 0):\n",
    "            r_cost, r_accuracy = sess.run([cost, accuracy], feed_dict= {X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1.0})\n",
    "            print(\"[{0} epoch, {1:0.2f}s pass] cost: {2:0.5f} , accuracy: {3:0.2f} %\".format(epoch, (time.time()-start_time), r_cost, r_accuracy*100))\n",
    "    \n",
    "    result = sess.run(accuracy, feed_dict= {X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1.0})\n",
    "    print(\"---Report---\")\n",
    "    print(\"Time: {0:0.2f}s , accuracy: {1:0.2f} %\".format((time.time()-start_time), result*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
