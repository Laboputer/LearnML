{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Learning with TensorFlow 1.0 (Python) !**\n",
    " > Deep Learning을 이해하기 위해 인터넷강의를 통해 공부하고, TensorFlow 1.0으로 직접 구현해보면서\n",
    " 학습한 내용을 토대로 작성되었다.\n",
    " 이 자료를 통해 기초적인 Machine Learning에 대한 개념과 Deep Learning을 이해하고, 실제 데이터에 대한 예측 모델을 구성해볼 수 있다.\n",
    " \n",
    "*참고자료* :  \n",
    "*[김성훈 님][https://github.com/hunkim]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Machine Learning Basic concepts\n",
    "![Machine Learning](Image/image1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #1. Check TF Version\n",
      "1.12.0\n",
      "\n",
      " #2. Hello Tensorflow\n",
      "b'Hello, TensorFlow!'\n",
      "\n",
      " #3. Computational Graph\n",
      "node1: Tensor(\"Const_10:0\", shape=(), dtype=float32)\n",
      "node2: Tensor(\"Const_11:0\", shape=(), dtype=float32)\n",
      "node3:  Tensor(\"Add_6:0\", shape=(), dtype=float32)\n",
      "sess.run(node1, node2):  [3.0, 4.0]\n",
      "sess.run(node3):  7.0\n",
      "\n",
      " #4. Placeholder\n",
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "# Getting Started With TensorFlow\n",
    "# https://www.tensorflow.org/get_started/get_started\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "## 1. Check TF Verison\n",
    "print(\" #1. Check TF Version\")\n",
    "print(tf.__version__)\n",
    "\n",
    "## 2. Hello Tensorflow\n",
    "# Create a constant op\n",
    "# This op is added as a node to the default graph\n",
    "print(\"\\n #2. Hello Tensorflow\")\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "# start a TF session\n",
    "sess = tf.Session()\n",
    "# run the op and get result\n",
    "print(sess.run(hello))\n",
    "\n",
    "## 3. Computational Graph\n",
    "print(\"\\n #3. Computational Graph\")\n",
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "node3 = tf.add(node1, node2)\n",
    "\n",
    "print(\"node1:\", node1)\n",
    "print(\"node2:\", node2)\n",
    "print(\"node3: \", node3)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(\"sess.run(node1, node2): \", sess.run([node1, node2]))\n",
    "print(\"sess.run(node3): \", sess.run(node3))\n",
    "\n",
    "## 4. Placeholder\n",
    "print(\"\\n #4. Placeholder\")\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, feed_dict={a: [1,3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "\n",
    "> Linear Regression은 종속 변수 Y와 한 개 이상의 독립 변수 X와의 선형 상관 관계를 모델링하는 회귀분석 기법이다.\n",
    "\n",
    "![Machine Learning](Image/image2.jpg)\n",
    "\n",
    "* **Hypothesis**\n",
    "  \n",
    "  Weight * Features 에 해당하는 모델을 구성한다.\n",
    "\n",
    "* **Cost Function**\n",
    "\n",
    "  Hypothesis에 할당된 Weight값에 의해 어떤 Cost(Loss)를 가지는지 계산하는 함수이며 이 Cost Function \n",
    "  이 최소화하는 Weight들을 검색하는 것이 우리의 목표이다.\n",
    "\n",
    "* **Gradient descent algorithm**\n",
    "\n",
    "  Cost를 최소화하기 위해 cost(W)에 대해 편미분한 값을 이용하여 최저점을 탐색하는 알고리즘\n",
    "\n",
    "* **Convex function**\n",
    "\n",
    "  볼록한 형태의 함수로, Local minimum에 빠지지 않고, 미분한 값이 0에 가까울수록 Global Minimum에 해당하는 값을 도출할 수 있는 형태이다. Cost Function이 이 형태를 띄어야 학습이 제대로 동작할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1 Linear Regression (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1-1. Linear Regression (Tesorflow). Learing Started.\n",
      "Step : 0, Cost : 28.97908, [W, b] = [-0.029, 0.341]\n",
      "Step : 50, Cost : 0.02526, [W, b] = [1.103, 0.729]\n",
      "Step : 100, Cost : 0.01801, [W, b] = [1.087, 0.787]\n",
      "Step : 150, Cost : 0.01283, [W, b] = [1.073, 0.835]\n",
      "Step : 200, Cost : 0.00915, [W, b] = [1.062, 0.877]\n",
      "Step : 250, Cost : 0.00652, [W, b] = [1.052, 0.911]\n",
      "Step : 300, Cost : 0.00465, [W, b] = [1.044, 0.941]\n",
      "Step : 350, Cost : 0.00331, [W, b] = [1.037, 0.966]\n",
      "Step : 400, Cost : 0.00236, [W, b] = [1.031, 0.987]\n",
      "Step : 450, Cost : 0.00168, [W, b] = [1.027, 1.004]\n",
      "Step : 500, Cost : 0.00120, [W, b] = [1.022, 1.019]\n",
      "[6.131133]\n",
      "[3.575126]\n",
      "[2.5527232 4.5975285]\n"
     ]
    }
   ],
   "source": [
    "# 1-1. Linear Regression (Tesorflow)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Traning Set\n",
    "X_data = [1, 2, 3, 4, 5]\n",
    "Y_data = [2.1, 3.1, 4.1, 5.1, 6.1]\n",
    "\n",
    "# Variable : Tensorflow상에서 학습되는 Weight로 Graph Run되면 자동으로 값을 찾는다.\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None])\n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "# Hyposthesis\n",
    "hypothesis = X * W + b\n",
    "# Linear regression Cost Function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"# 1-1. Linear Regression (Tesorflow). Learing Started.\")\n",
    "#Learning\n",
    "for step in range(501):\n",
    "    cost_val, w_val, b_val, _ = sess.run([cost,W,b,train], \n",
    "                                         feed_dict={X: X_data,\n",
    "                                                    Y: Y_data})\n",
    "    if(step % 50 == 0):\n",
    "        print(\"Step : %d, Cost : %.5f, [W, b] = [%.3f, %.3f]\" % (step, cost_val, w_val, b_val))\n",
    "\n",
    "# Testing our model\n",
    "print(sess.run(hypothesis, feed_dict={X:[5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X:[2.5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X:[1.5, 3.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2 Linear Regression (Cost Function이 Convex 할까?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1-2 Linear Regression (Cost Function이 Convex 할까?)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1.2 Linear Regression\n",
    "# Cost Function이 Convex function인지 확인하기\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X=[1,2,3]\n",
    "Y=[1,2,3]\n",
    "\n",
    "W=tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = X * W\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "W_val=[]\n",
    "cost_val=[]\n",
    "\n",
    "print(\"# 1-2 Linear Regression (Cost Function이 Convex 할까?)\")\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i*0.1\n",
    "    curr_cost, curr_W = sess.run([cost,W], feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "\n",
    "# Show the cost function\n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3 Linear Regression (Multi-variable은 어떻게 계산할까? - 행렬연산)\n",
    "![Multi-variable](Image/image3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1.3 Linear Regression(Multi-Variable) Learing Started.\n",
      "Step : 0, cost : 79654.64062, Hypothesis:  [[ -96.312805]\n",
      " [-118.1729  ]\n",
      " [-115.168945]\n",
      " [-125.842766]\n",
      " [ -90.363205]]\n",
      "Step : 200, cost : 3.64365, Hypothesis:  [[153.95853]\n",
      " [182.91168]\n",
      " [181.35086]\n",
      " [197.07372]\n",
      " [139.34601]]\n",
      "Step : 400, cost : 3.29232, Hypothesis:  [[153.81964]\n",
      " [183.00717]\n",
      " [181.30864]\n",
      " [197.04068]\n",
      " [139.47342]]\n",
      "Step : 600, cost : 2.97706, Hypothesis:  [[153.68811]\n",
      " [183.09756]\n",
      " [181.26865]\n",
      " [197.00928]\n",
      " [139.59413]]\n",
      "Step : 800, cost : 2.69411, Hypothesis:  [[153.56358]\n",
      " [183.18318]\n",
      " [181.2308 ]\n",
      " [196.97945]\n",
      " [139.70856]]\n",
      "Step : 1000, cost : 2.44019, Hypothesis:  [[153.44571]\n",
      " [183.26425]\n",
      " [181.195  ]\n",
      " [196.95111]\n",
      " [139.81702]]\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Linear Regression\n",
    "# Multi-Variable인 경우에도 학습하기\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[73, 80, 75], [93, 88, 93], [89, 91, 90], [96,98, 100], [73, 66, 70]]\n",
    "y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "# [None, 3] : 주어지는 데이터가 N개 이며, X 종류(Features)는  3개\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3]) # X1 ~ X3 : 3\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1]) # Y1 : 1\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train  = optimizer.minimize(cost)\n",
    "\n",
    "#launch the graph in a session\n",
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"# 1.3 Linear Regression(Multi-Variable) Learing Started.\")\n",
    "for step in range(1001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={X : x_data, Y : y_data })\n",
    "    if(step % 200 == 0):\n",
    "        print(\"Step : %d, cost : %.5f,\" % (step, cost_val),\"Hypothesis: \", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression (classification)\n",
    "\n",
    "> Logistic Regression은 일반적인 회귀 분석의 목표와 동일하게 종속 변수와 독립 변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용하는 것이다. Logistic Regression에서는 종속 변수가 범주형 데이터로 Classification 기법으로 생각하면 된다. Tensorflow에서는 각 범주형 데이터일 값을 구하는 Regression과 각 범주형 데이터의 확률로 표기하는 Classification으로 구할 것이다.\n",
    "\n",
    "![](Image/image4.jpg)\n",
    "\n",
    "* **Classification**\n",
    "\n",
    "  데이터를 분류함에 있어서 YES or No (1,0) 둘 중 하나를 분류하는 것을 Binary classification이라 불리며, N개의 범주로 분류되는 것을 Multinomial classfication이라고 한다.\n",
    "  \n",
    "* **Logistic Hypothesis**\n",
    "\n",
    "  Logictic은 Linear Hypothesis 모델을 그대로 이용할 경우 매우 큰 값/작은 값에 평균 값이 크게 달라지는 현상을 보이기 때문에, 데이터를 학습할 때 특이 케이스에 의해 동작이 잘 되지 않는다. 이때에도 잘 동작할 수 있게 하는 것이 매우 큰 이슈였는데, Sigmoid라는 함수를 한번 더 통과함으로써 효과적으로 동작할 수 있다. 이 값은 항상 0~1 사이의 값을 가진다.\n",
    "  \n",
    "* **Cost Function**\n",
    "\n",
    "  Cost Function 또한 그래프처럼 Convex 형태를 띄지 않아 학습이 되지 않는다. 이를 보완하기 위해 X가 지수에 있는 것을 상쇄하기 위해, log을 취한 형태로 Logistic을 위한 새로운 Cost Function을 사용한다. 이 Cost Function을 다시 Gradient decent algorithm을 적용하면 Cost를 최소화할 수 있다.\n",
    "  \n",
    "  \n",
    "  #  \n",
    "  \n",
    "\n",
    "![](Image/image5.jpg)\n",
    "\n",
    "* **Multinomial Classifiaction**\n",
    " \n",
    "  Multinomial Classification은 사실 Binary를 N번 구하는 것으로 구할 수 있다. 이를 행렬곱을 이용하면 쉽게 N번을 연산할 수 있게 된다. 각각의 Score를 계산할 수 있다.\n",
    "  \n",
    "* **Softmax**\n",
    "\n",
    "  Multinomial Classification에서 우리가 원하는 것은 Score가 아니라 각각의 확률값으로, 전체의 합은 1의 형태를 띄우고자함인데, 이는 Softmax로 구할 수 있다. 원리는 정확히 모르겠다.\n",
    "  \n",
    "* **Cross entropy**\n",
    "\n",
    "  결과값은 Softmax와 같다고 하는데, 아직 이해가 되지 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1 Logistic Regression (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 2-1 Logistic Regression (Tensorflow) Learing Started.\n",
      "Step :    0, Cost: 6.23049, Accuracy: 50.00 %\n",
      "Step : 1000, Cost: 0.38463, Accuracy: 83.33 %\n",
      "Step : 2000, Cost: 0.32684, Accuracy: 83.33 %\n",
      "Step : 3000, Cost: 0.28358, Accuracy: 83.33 %\n",
      "Step : 4000, Cost: 0.24952, Accuracy: 100.00 %\n",
      "Step : 5000, Cost: 0.22227, Accuracy: 100.00 %\n",
      "Step : 6000, Cost: 0.20015, Accuracy: 100.00 %\n",
      "Step : 7000, Cost: 0.18192, Accuracy: 100.00 %\n",
      "\n",
      " Hypothesis :  [[0.04546751]\n",
      " [0.17501704]\n",
      " [0.36895317]\n",
      " [0.7534366 ]\n",
      " [0.92061836]\n",
      " [0.974031  ]] \n",
      "Correct(Y) :  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      " Accuracy:  100.0\n"
     ]
    }
   ],
   "source": [
    "# 2-1 Logistic Regression (Tensorflow)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))\n",
    "hypothesis =  tf.sigmoid(tf.matmul(X, W) + b)  \n",
    "# Simplified cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train  = optimizer.minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5  => sigmoid를 통과한 값이 0.5보다 크면 1로 인식하게 한다.\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) * 100 # to %\n",
    "\n",
    "print(\"# 2-1 Logistic Regression (Tensorflow) Learing Started.\")\n",
    "#launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(7001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X : x_data, Y : y_data })\n",
    "        if(step % 1000 == 0):\n",
    "            a = sess.run([accuracy],feed_dict={X: x_data, Y:y_data})[0]\n",
    "            print(\"Step : %4d, Cost: %.5f, Accuracy: %.2f\" % (step, cost_val, a), \"%\")\n",
    "\n",
    "\n",
    "    #Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],feed_dict={X: x_data, Y:y_data})\n",
    "    print(\"\\n Hypothesis : \", h, \"\\nCorrect(Y) : \", c, \"\\n Accuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2 Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 2-2 Softmax Classification + Fancy Softmax. Learing Started.\n",
      "Step :    0, Cost: 4.26827, Accuracy: 37.50 %\n",
      "Step :  500, Cost: 0.47780, Accuracy: 75.00 %\n",
      "Step : 1000, Cost: 0.29798, Accuracy: 87.50 %\n",
      "Step : 1500, Cost: 0.20561, Accuracy: 100.00 %\n",
      "Step : 2000, Cost: 0.16757, Accuracy: 100.00 %\n",
      "[[1.4185148e-02 9.8580486e-01 1.0000229e-05]\n",
      " [7.8595561e-01 1.9688424e-01 1.7160088e-02]\n",
      " [2.6808090e-08 4.2461287e-04 9.9957532e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# 2-2 Softmax Classification + Fancy Softmax\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]] # one-hot encoding\n",
    "n_inputs = 4 \n",
    "n_classes = 3\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_inputs])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "W = tf.Variable(tf.random_normal([n_inputs, n_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([n_classes]), name='bias')\n",
    "\n",
    "## 1. Softmax\n",
    "# # tf.nn.softmax computes softmax activations\n",
    "# # softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "# hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "# # Cross entropy cost/loss\n",
    "# cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "## 2. Fancy Softmax with cross entropy\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels= y_data)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# accuracy\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(y_data, 1)) # [예측값, 실제값] 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32)) * 100 # %\n",
    "\n",
    "print(\"# 2-2 Softmax Classification + Fancy Softmax. Learing Started.\")\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        cost_val, _ = sess.run([cost, optimizer], feed_dict={X: x_data, Y:y_data})\n",
    "        if (step % 500) == 0:\n",
    "            a = sess.run(accuracy, feed_dict={X: x_data, Y:y_data})\n",
    "            print(\"Step : %4d, Cost: %.5f, Accuracy: %.2f\" % (step, cost_val, a), \"%\")\n",
    "\n",
    "    # Test & One-hot encoding\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0 , 1]]})\n",
    "    print(all, sess.run(tf.arg_max(all, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. NN (Neural Network)\n",
    "\n",
    "> Neural Network가 등장하게 된 배경과 Neural Network의 Layer를 여러층 두게 될 때 어떻게 학습을 시키는지, 그리고 Machine Learning을 함에 있어서 여태까지 어떤 이슈가 있었는지, 아직 해결안된 문제는 무엇인지 확인해보고자 한다. 먼저 Neural Network의 발전과정을 알아보고, 어떻게 동작하는지 확인해보자.\n",
    "\n",
    "![](Image/image6.jpg)\n",
    "![](Image/image7.jpg)\n",
    "\n",
    "* **Neuron (Activation Functions)**\n",
    "\n",
    " 우리의 뇌 신경계의 세포인 뉴런을 분석해보면 어떤 특정 전기신호가 Input으로 들어오게 되고 일정값이 넘어가면 활성화가 되는 것을 본따서 Network를 구성하였다. 이때 활성화가 되기 위한 조건을 판단하는 함수를 Activation Function이라고 한다.\n",
    " \n",
    "* **Perceptrons**\n",
    " \n",
    " Marvin Minsky(인공지능 분야 교수)는 Perceptrons 이라는 책을 통해 XOR 문제를 한가지 Layer로는 절대 풀 수 없음을 수학적으로 증명하고, Multi Layer를 이용하면 XOR 문제를 해결할 수 있으나, 1969년 당시 어느 누구도 각각의 Layer의 Weight를 컴퓨터로 계산할 수 없음을 밝혔다.\n",
    "  \n",
    "* **Backpropagation**\n",
    " \n",
    " 1974년도 Paul Werbos 라는 분이 Backpropagation을 소개하면서 Multi Layer의 Weight를 계산할 수 있음을 보였고, 주목받지 못하다가 1986년 Hinton이 다시 한번 발표하였는데 꽤 잘 동작하였다.\n",
    "  \n",
    "* **The Problem with Back-Propagation**\n",
    "   \n",
    " Back-Propagation을 이용하여 Multi Layer도 잘 학습되는 듯 하였으나, 적은 Layer에서는 잘 동작하지만 10개 이상 Deep 해질수록 신호가 약해지면서 학습이 제대로 되지 않는 문제가 발생했다. 이때 부터는 SVM, RandomForest 같은 다른 learning algorithm이 떠오르기 시작하였다.\n",
    "  \n",
    "* **Geoffrey Hinton's summary of findings up to today**\n",
    " \n",
    " 여태까지 Deep Layer가 잘 학습되지 않았던 이유 4가지를 들면서, 이를 어느정도 보완한 방법들이 나오자 Deep Learning은 잘 동작하기 시작했다. 간단하게 요약하면 Activation Function을 sigmoid로 그동안 잘 못 사용해왔고, 초기값 설정을 제대로 하지 않고 랜덤하게 셋팅한 것과 컴퓨터가 학습하기에 느렸고, DataSet이 너무 적었다고 말한다. 아래에서 자세하게 다뤄보자.\n",
    "\n",
    "#  \n",
    "\n",
    "![](Image/image8.jpg)\n",
    "![](Image/image9.jpg)\n",
    "\n",
    "* **Back Propagation (Chain rule)**\n",
    "\n",
    " 각각의 X값이 최종 Y값에 어떤 영향을 미치는 지를 구하는데 이용한다. 먼저 Y가 가장 가까운 Layer에서는 연산식을 알기 때문에 미분한 값을 구할 수 있으며, 그다음 가까운 Layer에서도 인접한 미분값을 구할 수 있는데, 이를 곱해주기만 하면 (Chain rule) 각각의 Weight들을 학습할 수 있다.\n",
    " \n",
    "* **Vanishing gradient**\n",
    "\n",
    " Layer가 매우 깊어지면 Back-Propagation을 하면서 Weight값이 사라지는 현상이다. 그 이유는 편미분한 값을 구할 때 sigmoid 함수로 0 ~ 1 사이의 값을 만드는데, 이를 반복하여 곱하면서 우연히 0에 가까운 값이 나오면 급격하게 0에 가까워지는 현상이 발생한다. 이를 보완하기 위해 Activation Function을 바꾸는데 단순하게 ReLU 함수를 사용하면 훨씬 잘 동작함을 알 수 있다. 이 외에도 여러가지 Activation Function들이 나오고 있다.\n",
    " \n",
    "* **Set the initial weight**\n",
    " \n",
    " Weight값을 랜덤하게 셋팅하는 것이 아니라 최대한 결과 데이터에 가깝도록 셋팅하자는 이슈이다. 이 이슈는 아직 해결되지 않고 계속 발전중인 분야이다. 하지만 지금도 어느정도 잘 동작하는 방법이 존재한다. 2006년에 RBM 이라는 모든 레이어의 인접한 2개 Layer 끼리만 학습하면서 각각의 Weight를 학습하는 아이디어인데 학습효과가 좋아서 실제 모두 Learning하는 것을 튜닝이라고 부를 정도이다. 하지만 구현방법이 조금 까다롭다.\n",
    " \n",
    "* **Xavier / He**\n",
    " \n",
    " RBM을 이용하지 않고 아주 단순한 방법으로도 효과가 좋은 방법이 있다. Fanin/out 값을 이용한 것인데 왜 동작하는지는 모르지만 구현하기 매우 쉽고, 학습효과도 매우 좋다. Xavier initialization, He의 방법을 이용해보자.\n",
    "  \n",
    "* **Dropout**\n",
    " \n",
    " Deep Learning 에서 너무 Deep/Wide 하게 Layer를 두면 Overfitting 되어 테스트 데이터에서는 잘 예측하지 못하는 현상이 발생하는데, 이 때 쓸 수 있는 방법으로 임의의 노드들을 학습하지 않도록 참여시키지 않는 것이다. 단, 최종적으로 예측모델을 사용할 때는 모든 노드를 사용한다. 아직 이유는 밝혀지지 않았지만 예측률이 좋아진다.\n",
    " \n",
    "* **Ensemble**\n",
    " \n",
    " 똑같은 형태의 모델들을 여러개 만들어 학습시키면 각각의 Weight가 다른데, 각각의 모델의 결과를 합산하여 좀 더 성능향상을 꾀하는 방법이다.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1 XOR Problem (Linear Regression, Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 3-1 XOR Problem with Linear Regression, Neural Network.\n",
      "# 1. Linear Regression. Learning Started..\n",
      "Step:    0, Cost: 0.72522, Accuracy: 75.00\n",
      "Step: 2000, Cost: 0.69315, Accuracy: 50.00\n",
      "Step: 4000, Cost: 0.69315, Accuracy: 50.00\n",
      "Step: 6000, Cost: 0.69315, Accuracy: 50.00\n",
      "Step: 8000, Cost: 0.69315, Accuracy: 50.00\n",
      "Step: 10000, Cost: 0.69315, Accuracy: 50.00\n",
      "\n",
      "# 2. Neural Network with 2 Layers. Learning Started..\n",
      "Step:    0, Cost: 0.72944, Accuracy: 50.00\n",
      "Step: 2000, Cost: 0.14233, Accuracy: 100.00\n",
      "Step: 4000, Cost: 0.02958, Accuracy: 100.00\n",
      "Step: 6000, Cost: 0.01435, Accuracy: 100.00\n",
      "Step: 8000, Cost: 0.00913, Accuracy: 100.00\n",
      "Step: 10000, Cost: 0.00660, Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "# 3-1 XOR Problem with Linear Regression, Neural Network\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "## 1. Linear Regression\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) * 100\n",
    "\n",
    "print(\"# 3-1 XOR Problem with Linear Regression, Neural Network.\")\n",
    "print(\"# 1. Linear Regression. Learning Started..\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, a, _ = sess.run([cost, accuracy, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: %4d, Cost: %.5f, Accuracy: %.2f\"% (step, cost_val, a))\n",
    "            \n",
    "## 2. Neural Network with 2 Layers\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), name=\"bias2\")\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) * 100\n",
    "    \n",
    "print(\"\\n# 2. Neural Network with 2 Layers. Learning Started..\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, a, _ = sess.run([cost, accuracy, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: %4d, Cost: %.5f, Accuracy: %.2f\"% (step, cost_val, a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NetworkModel' object has no attribute 'cost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-22ee96f9a6c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mcost_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step: %4d, Cost: %.5f, Accuracy: %.2f\"\u001b[0m\u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-22ee96f9a6c5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_data, y_data)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NetworkModel' object has no attribute 'cost'"
     ]
    }
   ],
   "source": [
    "# 3-1 XOR Problem with Linear Regression, Neural Network\n",
    "\n",
    "# 파이썬 클래스 공부한다음 다시 작성하자..\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "class NetworkModel:\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, [None, 2])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    # 1. Linear Regression\n",
    "    def _build_linear_regression(self):\n",
    "        tf.set_random_seed(777)  # for reproducibility\n",
    "        \n",
    "        with tf.variable_scope(self.name):\n",
    "            W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "            b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "            # Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "            hypothesis = tf.sigmoid(tf.matmul(self.X, W) + b)\n",
    "            # cost/loss function\n",
    "            cost = -tf.reduce_mean(self.Y * tf.log(hypothesis) + (1 - self.Y) * tf.log(1 - hypothesis))\n",
    "            train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "            \n",
    "            # Accuracy computation\n",
    "            # True if hypothesis>0.5 else False\n",
    "            predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, self.Y), dtype=tf.float32)) * 100\n",
    "    \n",
    "    def train(self, x_data, y_data):\n",
    "        return self.sess.run([self.cost, self.train], feed_dict={self.X: x_data, self.Y: y_data})\n",
    "    \n",
    "    def get_accuracy(self, x_test, y_test):\n",
    "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test})\n",
    "\n",
    " # 1. Linear Regression\n",
    "sess1 = tf.Session()\n",
    "linear = NetworkModel(sess1, \"LinearRegression\")\n",
    "linear._build_linear_regression()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = linear.train(x_data, y_data)\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: %4d, Cost: %.5f, Accuracy: %.2f\"% (step, cost_val, linear.get_accuracy(x_data, y_data)))\n",
    "            \n",
    "# 2. Neural Network with 2 Layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2 XOR with Deep-Wide Learning (Vanishing gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 3-2 XOR with Deep + Wide Learing (Vanish gradient). Learning Started..\n",
      "Step:    0, Cost: 1.78570, Accuracy: 50.00\n",
      "Step: 2000, Cost: 0.69259, Accuracy: 50.00\n",
      "Step: 4000, Cost: 0.68282, Accuracy: 75.00\n",
      "Step: 6000, Cost: 0.00730, Accuracy: 100.00\n",
      "Step: 8000, Cost: 0.00258, Accuracy: 100.00\n",
      "Step: 10000, Cost: 0.00153, Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "# 3-2 XOR with Deep + Wide Learing (Vanishing gradient)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "inputs = 15\n",
    "# 이거.. Input을 늘려서 더 Wide하게 하면.. 정확해지는데... Vanish는 Wide하게 하면 좋아지나..?\n",
    "# sigmoid는 오히려 잘 동작하고.. relu 쓰면 nan 나오는데요..? 이상하네\n",
    "\n",
    "# 10 Layer + 15 inputs every layers\n",
    "W1 = tf.Variable(tf.random_normal([2, inputs]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([inputs]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "# relu\n",
    "# layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([inputs]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([inputs]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([inputs]), name='bias4')\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "W5 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight5')\n",
    "b5 = tf.Variable(tf.random_normal([inputs]), name='bias5')\n",
    "layer5 = tf.sigmoid(tf.matmul(layer4, W5) + b5)\n",
    "\n",
    "W6 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight6')\n",
    "b6 = tf.Variable(tf.random_normal([inputs]), name='bias6')\n",
    "layer6 = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "W7 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight7')\n",
    "b7 = tf.Variable(tf.random_normal([inputs]), name='bias7')\n",
    "layer7 = tf.sigmoid(tf.matmul(layer6, W7) + b7)\n",
    "\n",
    "W8 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight8')\n",
    "b8 = tf.Variable(tf.random_normal([inputs]), name='bias8')\n",
    "layer8 = tf.sigmoid(tf.matmul(layer7, W8) + b8)\n",
    "\n",
    "W9 = tf.Variable(tf.random_normal([inputs, inputs]), name='weight9')\n",
    "b9 = tf.Variable(tf.random_normal([inputs]), name='bias9')\n",
    "layer9 = tf.sigmoid(tf.matmul(layer8, W9) + b9)\n",
    "\n",
    "W10 = tf.Variable(tf.random_normal([inputs, 1]), name='weight10')\n",
    "b10 = tf.Variable(tf.random_normal([1]), name='bias10')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer9, W10) + b10)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) * 100\n",
    "    \n",
    "print(\"# 3-2 XOR with Deep + Wide Learing (Vanish gradient). Learning Started..\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, a, _ = sess.run([cost, accuracy, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: %4d, Cost: %.5f, Accuracy: %.2f\"% (step, cost_val, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Bonus) 실제 데이터를 사용하기 전 알아야할 개념\n",
    "\n",
    "![](Image/image10.jpg)\n",
    "\n",
    "* **Learning Rate**\n",
    "\n",
    "  Gradient decent에서 변경하는 값의 비율로 너무 크면 NaN으로 값이 튀거나 너무 작으면 Local Minimum에 빠져 Cost가 최소화되지 않는 현상이 발생할 수 있다.\n",
    "\n",
    "* **Data preprocessing (Regularization)**\n",
    "\n",
    "  데이터가 너무 편차가 크게 차이가 나는 경우에는 학습이 제대로 되지 않을 수 있는데 이때는 데이터를 정규분포화한다.\n",
    "  \n",
    "* **Training Set / Test Set**\n",
    "\n",
    "  데이터를 학습할 때는 반드시 학습데이터와 일정비율(10~20%)은 테스트 데이터로 분리하여야 모델이 제대로 동작하는지 평가할 수 있다. Validation Set을 한번 더 나눠 학습데이터를 튜닝할 때 이용하기도 한다. 또한 데이터가 너무 많을 경우에 메모리에 올릴 수 없는 상황에서는 Online learning으로 쪼개서 학습하도록 하고, epoch과 batch_size를 결정하여 데이터를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (실습1) 다양한 모델을 이용한 MNIST 예측하기\n",
    "\n",
    " > 이번 실습에서는 1~3 챕터에서 배웠던 내용을 숫자 0~9까지 손글씨로 입력한 이미지 데이터를 실제로 어떤 값을 나타내는지 예측하는 모델을 만들어보고자 한다. 여러가지 모델을 적용하여 최대한 예측률을 높여보자.\n",
    "\n",
    "![](Image/image11.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels(one-hot) :  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "number :  7\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Tensorflow에 있는 MNIST 데이터를 이용하였음.\n",
    "# image는 784 (28*28)개의 0.0~1.0 사이의 값으로 표현된 이미지\n",
    "# label은 10개 (0~9) 까지의 class로 one_hot 데이터 형식\n",
    "# train은 55000개, test는 10000개\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(np.shape(mnist.train.images))\n",
    "print(np.shape(mnist.train.labels))\n",
    "print(np.shape(mnist.test.images))\n",
    "print(np.shape(mnist.test.labels))\n",
    "\n",
    "# 데이터 구경하기\n",
    "plt.imshow(mnist.test.images[0].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n",
    "print(\"labels(one-hot) : \", mnist.test.labels[0])\n",
    "print(\"number : \", np.argmax(mnist.test.labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습1 에 공통된 코드인 러닝하는 함수를 따로 분리하였으니, 아래 부분을 코드를 최초로 실행시킨 후에\n",
    "모델만 만들어서 코드를 실행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-5fa8f44d4ff0>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\khj\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "learningRate = 0.01\n",
    "trainingEpochs = 10\n",
    "batchSize = 100\n",
    "\n",
    "def set_hyper_parameters(learning_rate, training_epochs, batch_size):\n",
    "    global learningRate\n",
    "    global trainingEpochs\n",
    "    global batchSize\n",
    "    learningRate = learning_rate\n",
    "    trainingEpochs = training_epochs\n",
    "    batchSize = batch_size\n",
    "\n",
    "def get_accuracy(sess, x_data, y_data):\n",
    "    return sess.run(accuracy, feedict = {X: x_data, Y: y_data})\n",
    "\n",
    "def run_train_model(sess):\n",
    "    print(\"===== Learning Stated... =====\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(trainingEpochs + 1):\n",
    "        total_batch = int(mnist.train.num_examples / batchSize)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batchSize)\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "        train_cost, train_acc = sess.run([cost, accuracy], feed_dict={X: mnist.train.images, Y: mnist.train.labels})\n",
    "        test_cost, test_acc = sess.run([cost, accuracy], feed_dict={X: mnist.test.images, Y: mnist.test.labels})\n",
    "\n",
    "        if((epoch % (trainingEpochs/5)) == 0):\n",
    "            print(\"[{0} epoch, {1:0.2f}s pass] [TEST_DATA] Cost: {2:0.5f} , Accuracy: {3:0.3f} %\"\n",
    "            .format(epoch, (time.time()-start_time), test_cost, test_acc * 100))\n",
    "    \n",
    "    train_cost, train_acc = sess.run([cost, accuracy], feed_dict={X: mnist.train.images, Y: mnist.train.labels})\n",
    "    test_cost, test_acc = sess.run([cost, accuracy], feed_dict={X: mnist.test.images, Y: mnist.test.labels})\n",
    "\n",
    "    print(\"===== Learning Finished... =====\")\n",
    "    print(\"[Hyper Parameters] Learing Rate: %f, epoch: %d, batch_size: %d\" % (learningRate, trainingEpochs, batchSize))\n",
    "    print(\"[Accuracy gap: %.3f per, Learning time: %.2fs]\" % (((train_acc-test_acc) * 100), (time.time()-start_time)))\n",
    "    print(\"[TRAIN_DATA] Cost: {0:0.5f} , Accuracy: {1:0.3f} %\".format(train_cost, train_acc * 100))\n",
    "    print(\"[TEST_DATA] Cost: {0:0.5f} , Accuracy: {1:0.3f} %\".format(test_cost, test_acc * 100))\n",
    "\n",
    "# [주의사항] \n",
    "# Evaluation def 을 사용하기 위해서는 다음과 같은 Tensor를 모두 정의해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습1-1) Logistic Classification (Regression)\n",
    "이건 동작을 안하네..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "=== Learning Started ===\n",
      "step : 0000, cost: nan, Accuracy: 90.000 %\n",
      "step : 0001, cost: nan, Accuracy: 90.000 %\n",
      "step : 0002, cost: nan, Accuracy: 90.000 %\n",
      "step : 0003, cost: nan, Accuracy: 90.000 %\n",
      "step : 0004, cost: nan, Accuracy: 90.000 %\n",
      "step : 0005, cost: nan, Accuracy: 90.000 %\n",
      "step : 0006, cost: nan, Accuracy: 90.000 %\n",
      "step : 0007, cost: nan, Accuracy: 90.000 %\n",
      "step : 0008, cost: nan, Accuracy: 90.000 %\n",
      "step : 0009, cost: nan, Accuracy: 90.000 %\n",
      "step : 0010, cost: nan, Accuracy: 90.000 %\n",
      "step : 0011, cost: nan, Accuracy: 90.000 %\n",
      "step : 0012, cost: nan, Accuracy: 90.000 %\n",
      "step : 0013, cost: nan, Accuracy: 90.000 %\n",
      "step : 0014, cost: nan, Accuracy: 90.000 %\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train = mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "\n",
    "# 1. Logistic (regression) classification\n",
    "\n",
    "# Hyper Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "n_features = 784\n",
    "n_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_features, n_classes]))\n",
    "b = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Hypothesis using sigmoid : tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "#hypothesis = tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))\n",
    "# Simplified cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# logits = tf.matmul(X,W) + b\n",
    "# cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_train) #label must be one-hot\n",
    "# cost = tf.reduce_mean(cost_i)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) * 100\n",
    "\n",
    "#launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"=== Learning Started ===\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        current_cost, acc = sess.run([cost, accuracy], feed_dict={X: X_train, Y: Y_train})\n",
    "        print('step : %04d, cost: %.5f, Accuracy: %.3f' % (epoch, current_cost, acc), \"%\")\n",
    "            \n",
    "    print('Learning finished.')\n",
    "    # Test the model using test sets\n",
    "    #print(\"Accuracy: %.3f \" % accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습1-2) Logistic Classification (Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Learning Stated... =====\n",
      "[0 epoch, 0.62s pass] [TEST_DATA] Cost: 1.24081 , Accuracy: 74.940 %\n",
      "[10 epoch, 4.63s pass] [TEST_DATA] Cost: 0.49364 , Accuracy: 88.260 %\n",
      "[20 epoch, 8.62s pass] [TEST_DATA] Cost: 0.41648 , Accuracy: 89.870 %\n",
      "[30 epoch, 12.54s pass] [TEST_DATA] Cost: 0.38182 , Accuracy: 90.370 %\n",
      "[40 epoch, 16.58s pass] [TEST_DATA] Cost: 0.35793 , Accuracy: 90.720 %\n",
      "[50 epoch, 20.59s pass] [TEST_DATA] Cost: 0.34203 , Accuracy: 91.000 %\n",
      "===== Learning Finished... =====\n",
      "[Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
      "[Accuracy gap: 0.369 per, Learning time: 20.65s]\n",
      "[TRAIN_DATA] Cost: 0.32351 , Accuracy: 91.369 %\n",
      "[TEST_DATA] Cost: 0.34203 , Accuracy: 91.000 %\n"
     ]
    }
   ],
   "source": [
    "# 실습 1-2) Logistic classification (Softmax classfier)\n",
    "n_features = 784\n",
    "n_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "W = tf.Variable(tf.random_normal([n_features, n_classes]))\n",
    "b = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "set_hyper_parameters(learning_rate = 0.1, training_epochs = 50, batch_size = 100)\n",
    "\n",
    "# case 1. Cross entropy\n",
    "#logits = tf.matmul(X,W) + b\n",
    "#hypothesis = tf.nn.softmax(logits)\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) \n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# case 2. fancy Softmax by logits\n",
    "logits = tf.matmul(X,W) + b\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learningRate).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run with def\n",
    "run_train_model(sess)\n",
    "\n",
    "# ## Epoch / Batch_Size 변경\n",
    "\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 15, batch_size: 100\n",
    "# [Accuracy gap: -0.485 per, Learning time: 6.75s]\n",
    "# [TRAIN_DATA] Cost: 0.45354 , Accuracy: 88.955 %\n",
    "# [TEST_DATA] Cost: 0.44991 , Accuracy: 89.440 %\n",
    "\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 0.499 per, Learning time: 20.62s]\n",
    "# [TRAIN_DATA] Cost: 0.32619 , Accuracy: 91.429 %\n",
    "# [TEST_DATA] Cost: 0.35714 , Accuracy: 90.930 %\n",
    "\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 500\n",
    "# [Accuracy gap: -0.337 per, Learning time: 16.11s]\n",
    "# [TRAIN_DATA] Cost: 0.51811 , Accuracy: 87.973 %\n",
    "# [TEST_DATA] Cost: 0.51571 , Accuracy: 88.310 %\n",
    "\n",
    "\n",
    "# ## Learning Rate 변경\n",
    "\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 0.62s pass] [TEST_DATA] Cost: 5.48238 , Accuracy: 29.110 %\n",
    "# [10 epoch, 4.62s pass] [TEST_DATA] Cost: 1.08241 , Accuracy: 77.560 %\n",
    "# [20 epoch, 8.62s pass] [TEST_DATA] Cost: 0.82483 , Accuracy: 82.290 %\n",
    "# [30 epoch, 12.60s pass] [TEST_DATA] Cost: 0.71857 , Accuracy: 84.290 %\n",
    "# [40 epoch, 16.63s pass] [TEST_DATA] Cost: 0.65594 , Accuracy: 85.490 %\n",
    "# [50 epoch, 20.64s pass] [TEST_DATA] Cost: 0.61241 , Accuracy: 86.260 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.010000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: -0.555 per, Learning time: 20.70]\n",
    "# [TRAIN_DATA] Cost: 0.63007 , Accuracy: 85.705 %\n",
    "# [TEST_DATA] Cost: 0.61241 , Accuracy: 86.260 %\n",
    "\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 0.62s pass] [TEST_DATA] Cost: 1.24081 , Accuracy: 74.940 %\n",
    "# [10 epoch, 4.63s pass] [TEST_DATA] Cost: 0.49364 , Accuracy: 88.260 %\n",
    "# [20 epoch, 8.62s pass] [TEST_DATA] Cost: 0.41648 , Accuracy: 89.870 %\n",
    "# [30 epoch, 12.54s pass] [TEST_DATA] Cost: 0.38182 , Accuracy: 90.370 %\n",
    "# [40 epoch, 16.58s pass] [TEST_DATA] Cost: 0.35793 , Accuracy: 90.720 %\n",
    "# [50 epoch, 20.59s pass] [TEST_DATA] Cost: 0.34203 , Accuracy: 91.000 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 0.369 per, Learning time: 20.65s]\n",
    "# [TRAIN_DATA] Cost: 0.32351 , Accuracy: 91.369 %\n",
    "# [TEST_DATA] Cost: 0.34203 , Accuracy: 91.000 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습1-3) Deep + Wide NN\n",
    "Deep + Wide NN (5 Layers + 128 hide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Learning Stated... =====\n",
      "[0 epoch, 1.31s pass] [TEST_DATA] Cost: 1.01420 , Accuracy: 66.800 %\n",
      "[10 epoch, 12.28s pass] [TEST_DATA] Cost: 0.38927 , Accuracy: 87.810 %\n",
      "[20 epoch, 23.17s pass] [TEST_DATA] Cost: 0.32992 , Accuracy: 89.900 %\n",
      "[30 epoch, 33.78s pass] [TEST_DATA] Cost: 0.29821 , Accuracy: 91.060 %\n",
      "[40 epoch, 44.54s pass] [TEST_DATA] Cost: 0.28224 , Accuracy: 91.620 %\n",
      "[50 epoch, 55.63s pass] [TEST_DATA] Cost: 0.27328 , Accuracy: 92.090 %\n",
      "===== Learning Finished... =====\n",
      "[Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
      "[Accuracy gap: 4.721 per, Learning time: 55.82s]\n",
      "[TRAIN_DATA] Cost: 0.11593 , Accuracy: 96.811 %\n",
      "[TEST_DATA] Cost: 0.27328 , Accuracy: 92.090 %\n"
     ]
    }
   ],
   "source": [
    "n_features = 784\n",
    "n_classes = 10\n",
    "hide = 128\n",
    "\n",
    "set_hyper_parameters(learning_rate = 0.1, training_epochs = 50, batch_size = 100)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_features, hide]), name='weight1')\n",
    "b = tf.Variable(tf.random_normal([hide]), name='bias1')\n",
    "\n",
    "H_W1 = tf.Variable(tf.random_normal([hide, hide]), name='weight2')\n",
    "H_b1 = tf.Variable(tf.random_normal([hide]), name='bias2')\n",
    "\n",
    "H_W2 = tf.Variable(tf.random_normal([hide, hide]), name='weight3')\n",
    "H_b2 = tf.Variable(tf.random_normal([hide]), name='bias3')\n",
    "\n",
    "H_W3 = tf.Variable(tf.random_normal([hide, hide]), name='weight4')\n",
    "H_b3 = tf.Variable(tf.random_normal([hide]), name='bias4')\n",
    "\n",
    "last_W = tf.Variable(tf.random_normal([hide, n_classes]), name='weight5')\n",
    "last_b = tf.Variable(tf.random_normal([n_classes]), name='bias5')\n",
    "\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, H_W1) + H_b1)\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, H_W2) + H_b2)\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3, H_W3) + H_b3)\n",
    "\n",
    "# fancy Softmax by logits\n",
    "logits = tf.matmul(layer4, last_W) + last_b\n",
    "\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learningRate).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run with def\n",
    "run_train_model(sess)\n",
    "\n",
    "# # 1.3 기본\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 1.38s pass] [TEST_DATA] Cost: 0.92668 , Accuracy: 69.830 %\n",
    "# [10 epoch, 12.39s pass] [TEST_DATA] Cost: 0.37498 , Accuracy: 88.140 %\n",
    "# [20 epoch, 23.23s pass] [TEST_DATA] Cost: 0.31178 , Accuracy: 90.270 %\n",
    "# [30 epoch, 34.02s pass] [TEST_DATA] Cost: 0.28405 , Accuracy: 91.190 %\n",
    "# [40 epoch, 44.73s pass] [TEST_DATA] Cost: 0.26554 , Accuracy: 91.880 %\n",
    "# [50 epoch, 55.55s pass] [TEST_DATA] Cost: 0.25685 , Accuracy: 92.380 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 4.527 per, Learning time: 55.74s]\n",
    "# [TRAIN_DATA] Cost: 0.11009 , Accuracy: 96.907 %\n",
    "# [TEST_DATA] Cost: 0.25685 , Accuracy: 92.380 %\n",
    "\n",
    "# # 1.3 + AdamOptimizer\n",
    "\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 1.26s pass] [TEST_DATA] Cost: 0.68084 , Accuracy: 77.960 %\n",
    "# [10 epoch, 12.62s pass] [TEST_DATA] Cost: 0.28464 , Accuracy: 91.250 %\n",
    "# [20 epoch, 23.67s pass] [TEST_DATA] Cost: 0.24727 , Accuracy: 92.730 %\n",
    "# [30 epoch, 34.70s pass] [TEST_DATA] Cost: 0.24068 , Accuracy: 93.110 %\n",
    "# [40 epoch, 45.59s pass] [TEST_DATA] Cost: 0.24324 , Accuracy: 93.570 %\n",
    "# [50 epoch, 56.54s pass] [TEST_DATA] Cost: 0.25110 , Accuracy: 93.630 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.100000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 5.706 per, Learning time: 56.73s]\n",
    "# [TRAIN_DATA] Cost: 0.03375 , Accuracy: 99.336 %\n",
    "# [TEST_DATA] Cost: 0.25110 , Accuracy: 93.630 %\n",
    "\n",
    "\n",
    "# # 1.3 ReLU + Xavier (ReLU, Xavier를 따로 하면 학습이 안되네..?)\n",
    "\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 1.19s pass] [TEST_DATA] Cost: 0.62437 , Accuracy: 82.610 %\n",
    "# [10 epoch, 11.57s pass] [TEST_DATA] Cost: 0.18562 , Accuracy: 94.560 %\n",
    "# [20 epoch, 22.19s pass] [TEST_DATA] Cost: 0.13110 , Accuracy: 96.270 %\n",
    "# [30 epoch, 32.63s pass] [TEST_DATA] Cost: 0.10670 , Accuracy: 96.940 %\n",
    "# [40 epoch, 43.46s pass] [TEST_DATA] Cost: 0.09446 , Accuracy: 97.150 %\n",
    "# [50 epoch, 54.02s pass] [TEST_DATA] Cost: 0.08862 , Accuracy: 97.360 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.010000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 1.511 per, Learning time: 54.21s]\n",
    "# [TRAIN_DATA] Cost: 0.04265 , Accuracy: 98.871 %\n",
    "# [TEST_DATA] Cost: 0.08862 , Accuracy: 97.360 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습1-4) ReLU, Xavier, Dropout, AdamOptimizer\n",
    "ReLU + Xavier + Dropout 0.7 + AdamOptimizer를 # 1.3 과 동일한 모델에 적용해본 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Learning Stated... =====\n",
      "[0 epoch, 2.09s pass] [TEST_DATA] Cost: 0.32104 , Accuracy: 90.930 %\n",
      "[10 epoch, 22.67s pass] [TEST_DATA] Cost: 0.17034 , Accuracy: 95.320 %\n",
      "[20 epoch, 41.70s pass] [TEST_DATA] Cost: 0.17134 , Accuracy: 95.750 %\n",
      "[30 epoch, 60.58s pass] [TEST_DATA] Cost: 0.15388 , Accuracy: 96.290 %\n",
      "[40 epoch, 79.49s pass] [TEST_DATA] Cost: 0.16610 , Accuracy: 96.340 %\n",
      "[50 epoch, 98.45s pass] [TEST_DATA] Cost: 0.19230 , Accuracy: 96.250 %\n",
      "===== Learning Finished... =====\n",
      "[Hyper Parameters] Learing Rate: 0.002000, epoch: 50, batch_size: 100\n",
      "[Accuracy gap: 1.546 per, Learning time: 98.80s]\n",
      "[TRAIN_DATA] Cost: 0.07849 , Accuracy: 97.816 %\n",
      "[TEST_DATA] Cost: 0.18003 , Accuracy: 96.270 %\n"
     ]
    }
   ],
   "source": [
    "n_features = 784\n",
    "n_classes = 10\n",
    "hide = 128\n",
    "keep_prob = 0.7\n",
    "\n",
    "set_hyper_parameters(learning_rate = 0.002, training_epochs = 50, batch_size = 100)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "W = tf.get_variable('weight1', shape = [n_features, hide], \n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([hide]), name='bias1')\n",
    "\n",
    "H_W1 = tf.get_variable('weight2', shape = [hide, hide], \n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "H_b1 = tf.Variable(tf.random_normal([hide]), name='bias2')\n",
    "\n",
    "H_W2 = tf.get_variable('weight3', shape = [hide, hide],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "H_b2 = tf.Variable(tf.random_normal([hide]), name='bias3')\n",
    "\n",
    "H_W3 = tf.get_variable('weight4', shape = [hide, hide],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "H_b3 = tf.Variable(tf.random_normal([hide]), name='bias4')\n",
    "\n",
    "last_W = tf.get_variable('weight5', shape = [hide, n_classes],\n",
    "                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "last_b = tf.Variable(tf.random_normal([n_classes]), name='bias5')\n",
    "\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W) + b)\n",
    "layer1 = tf.nn.dropout(layer1, keep_prob=keep_prob)\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, H_W1) + H_b1)\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob=keep_prob)\n",
    "layer3 = tf.nn.relu(tf.matmul(layer2, H_W2) + H_b2)\n",
    "layer3 = tf.nn.dropout(layer3, keep_prob=keep_prob)\n",
    "layer4 = tf.nn.relu(tf.matmul(layer3, H_W3) + H_b3)\n",
    "layer4 = tf.nn.dropout(layer4, keep_prob=keep_prob)\n",
    "\n",
    "# fancy Softmax by logits\n",
    "logits = tf.matmul(layer4, last_W) + last_b # dropout 1.0\n",
    "\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y) #label must be one-hot\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learningRate).minimize(cost)\n",
    "\n",
    "# Evaluation Model\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run with def\n",
    "run_train_model(sess)\n",
    "\n",
    "# # 1.4 ReLU + Xavier + AdamOptimizer\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 1.25s pass] [TEST_DATA] Cost: 0.16134 , Accuracy: 95.200 %\n",
    "# [10 epoch, 12.19s pass] [TEST_DATA] Cost: 0.11121 , Accuracy: 97.470 %\n",
    "# [20 epoch, 23.43s pass] [TEST_DATA] Cost: 0.12800 , Accuracy: 97.490 %\n",
    "# [30 epoch, 34.83s pass] [TEST_DATA] Cost: 0.12125 , Accuracy: 97.680 %\n",
    "# [40 epoch, 46.38s pass] [TEST_DATA] Cost: 0.13925 , Accuracy: 97.900 %\n",
    "# [50 epoch, 58.11s pass] [TEST_DATA] Cost: 0.14305 , Accuracy: 98.110 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.001000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 1.854 per, Learning time: 58.30s]\n",
    "# [TRAIN_DATA] Cost: 0.00121 , Accuracy: 99.964 %\n",
    "# [TEST_DATA] Cost: 0.14305 , Accuracy: 98.110 %\n",
    "\n",
    "\n",
    "# # 1.4 RELU + Xavier + AdamOptimizer + Dropout 0.7\n",
    "# # 여기서는 별로 안좋아졌는데.. 체감상 Overfitting되는 경우에 좋아질 것 같다.\n",
    "# # 지금 모델이 생각보다 Deep, Wide가 잘 맞는것으로 보임\n",
    "\n",
    "# ===== Learning Stated... =====\n",
    "# [0 epoch, 2.09s pass] [TEST_DATA] Cost: 0.32104 , Accuracy: 90.930 %\n",
    "# [10 epoch, 22.67s pass] [TEST_DATA] Cost: 0.17034 , Accuracy: 95.320 %\n",
    "# [20 epoch, 41.70s pass] [TEST_DATA] Cost: 0.17134 , Accuracy: 95.750 %\n",
    "# [30 epoch, 60.58s pass] [TEST_DATA] Cost: 0.15388 , Accuracy: 96.290 %\n",
    "# [40 epoch, 79.49s pass] [TEST_DATA] Cost: 0.16610 , Accuracy: 96.340 %\n",
    "# [50 epoch, 98.45s pass] [TEST_DATA] Cost: 0.19230 , Accuracy: 96.250 %\n",
    "# ===== Learning Finished... =====\n",
    "# [Hyper Parameters] Learing Rate: 0.002000, epoch: 50, batch_size: 100\n",
    "# [Accuracy gap: 1.546 per, Learning time: 98.80s]\n",
    "# [TRAIN_DATA] Cost: 0.07849 , Accuracy: 97.816 %\n",
    "# [TEST_DATA] Cost: 0.18003 , Accuracy: 96.270 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CNN (Convolutional N.N.)\n",
    "### (실습2) CNN을 이용하여 MNIST 이미지 인식 향상시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 11 MNIST and Convolutional Neural Network\n",
    "\n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
    "\n",
    "# Final FC 7x7x64 inputs -> 10 outputs\n",
    "W3 = tf.get_variable(\"W3\", shape=[7 * 7 * 64, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L2_flat, W3) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RNN (Recurrent N.N.)\n",
    "### (실습3) RNN을 이용한 문자 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Softmax classifier Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
